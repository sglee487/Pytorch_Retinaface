{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "import torchvision.models._utils as _utils\n",
    "\n",
    "mov3lg = torchvision.models.mobilenet_v3_large()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): ConvBNActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mov3lg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 64 128 256\n",
    "# 64 128 256 512\n",
    "# 16 24 40 80 112 160 960\n",
    "\n",
    "# 40 80 160\n",
    "body5 = _utils.IntermediateLayerGetter(mov3lg.features, {'6':0, '10':1, '15':2})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "IntermediateLayerGetter(\n  (0): ConvBNActivation(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n    (2): Hardswish()\n  )\n  (1): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (2): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (3): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (4): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (5): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (6): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (7): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (8): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n        (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (9): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (10): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (11): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (12): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (13): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (14): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (15): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from data import WiderFaceDetection, detection_collate, preproc, cfg_mnet, cfg_re50\n",
    "\n",
    "img_dim = 840\n",
    "rgb_mean = (104, 117, 123) # bgr order\n",
    "\n",
    "dataset = WiderFaceDetection( './data/widerface/train/label.txt',preproc(img_dim, rgb_mean))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "batch_iterator = iter(data.DataLoader(dataset, 2, shuffle=True, num_workers=4, collate_fn=detection_collate))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "images, targets = next(batch_iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 840, 840])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 0.6174,  0.4787,  0.6617,  0.5268,  1.0000,  0.0000,  1.0000,  0.0000,\n           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000, -1.0000],\n         [ 0.6599,  0.4584,  0.7061,  0.5028,  1.0000,  0.0000,  1.0000,  0.0000,\n           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000, -1.0000],\n         [ 0.8503,  0.4566,  0.8909,  0.4991,  0.8634,  0.4711,  0.8792,  0.4722,\n           0.8721,  0.4803,  0.8658,  0.4874,  0.8775,  0.4877,  1.0000],\n         [ 0.8835,  0.4658,  0.9168,  0.5046,  1.0000,  0.0000,  1.0000,  0.0000,\n           1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000, -1.0000]]),\n tensor([[0.3858, 0.0549, 0.8742, 0.6752, 0.6035, 0.2397, 0.8014, 0.2707, 0.7160,\n          0.4027, 0.5530, 0.4570, 0.7626, 0.4881, 1.0000]])]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([(0,\n              tensor([[[[ 1.1721e+00,  6.9228e-02, -3.9747e-01,  ...,  8.7818e-01,\n                         -1.1011e-01,  2.8548e-01],\n                        [ 4.7327e-01, -1.0635e+00,  2.7001e-02,  ..., -8.0862e-01,\n                         -7.9550e-01, -1.6185e-01],\n                        [ 1.1909e+00, -2.7725e-01,  6.9662e-01,  ...,  5.5011e-01,\n                          1.0003e+00,  6.8787e-01],\n                        ...,\n                        [-4.8151e-01,  5.1347e-02, -1.1363e+00,  ...,  1.2122e-03,\n                         -4.8677e-01, -2.3148e-01],\n                        [ 9.6476e-01,  6.3809e-01,  1.6118e+00,  ..., -4.2627e-02,\n                         -9.9582e-02,  4.4157e-01],\n                        [ 9.6395e-01,  1.5906e+00,  1.0402e+00,  ..., -7.9937e-01,\n                         -8.5396e-01,  1.9993e-01]],\n              \n                       [[-8.7627e-01, -5.4330e-01, -2.5792e-02,  ..., -1.8612e-01,\n                         -6.4415e-02, -1.1243e+00],\n                        [-2.5136e-01, -4.8535e-01,  5.4443e-01,  ...,  5.4259e-01,\n                          7.8116e-01,  3.4117e-01],\n                        [-1.4178e+00, -8.5801e-01,  1.0186e+00,  ..., -5.5463e-02,\n                          9.2877e-02,  1.1118e+00],\n                        ...,\n                        [ 2.6230e-01,  1.4150e+00, -1.9990e-01,  ..., -2.0833e-02,\n                         -5.9561e-01, -1.0818e-01],\n                        [-5.4785e-01, -1.7448e+00,  1.5543e+00,  ..., -7.2155e-01,\n                         -1.2494e+00, -8.2264e-01],\n                        [-9.2908e-01, -1.5372e+00, -3.4183e+00,  ..., -3.8194e-01,\n                         -4.8590e-01, -9.9215e-01]],\n              \n                       [[-2.0940e-01, -1.0195e+00, -1.0586e+00,  ..., -4.5160e-01,\n                          1.9782e-01,  1.2470e-02],\n                        [ 2.5020e-01,  9.4780e-01,  1.3655e+00,  ...,  6.8982e-01,\n                          3.3426e-01,  1.2425e+00],\n                        [-8.9640e-02,  1.1651e-01,  2.4986e-01,  ...,  1.4733e-01,\n                          3.0400e-01,  2.6642e-01],\n                        ...,\n                        [-5.8508e-01,  6.6090e-01, -2.3515e-01,  ...,  2.5706e-01,\n                          3.0195e-01, -4.8154e-01],\n                        [-6.8679e-01, -9.0359e-01, -8.4630e-01,  ...,  2.0000e-01,\n                         -9.1194e-02,  9.0681e-02],\n                        [ 4.9514e-01,  1.6723e+00,  1.9171e+00,  ...,  5.0900e-02,\n                          1.9158e-01,  7.0022e-02]],\n              \n                       ...,\n              \n                       [[-2.1618e+00, -1.1193e+00, -2.7137e+00,  ..., -2.0935e+00,\n                         -7.5401e-01, -6.3502e-01],\n                        [-2.3586e+00, -1.1661e+00, -1.8013e+00,  ..., -9.4699e-01,\n                         -1.2291e+00, -8.1769e-01],\n                        [-1.0303e+00, -2.2719e-01,  4.1982e-01,  ...,  4.9353e-01,\n                          5.3990e-01,  5.7499e-01],\n                        ...,\n                        [-1.4189e+00,  1.4990e+00,  1.1591e+00,  ...,  1.3413e-01,\n                         -1.3431e-01,  3.2896e-01],\n                        [-9.6739e-01,  1.0597e+00,  3.5422e+00,  ..., -4.2319e-01,\n                         -9.3573e-01, -5.7952e-01],\n                        [-1.4164e-01,  1.3100e+00,  2.5119e+00,  ...,  2.7821e-01,\n                          4.6052e-01, -5.4587e-01]],\n              \n                       [[ 5.6649e-01,  1.1662e+00,  9.2817e-01,  ...,  1.0175e+00,\n                          9.0750e-01,  1.7660e+00],\n                        [ 3.6170e-01,  9.5654e-01, -5.1820e-01,  ..., -5.4050e-01,\n                          1.8852e-02,  7.1536e-01],\n                        [ 1.1596e+00,  5.7209e-01,  1.8089e+00,  ...,  1.3506e+00,\n                          1.4309e+00,  8.2315e-01],\n                        ...,\n                        [ 1.3010e+00,  4.5754e-01,  5.6102e-01,  ...,  6.5244e-01,\n                          7.9862e-01,  1.2213e+00],\n                        [-3.1050e-01,  2.8720e-01, -4.0178e-01,  ...,  4.0922e-01,\n                          7.9655e-01,  1.6221e+00],\n                        [ 4.0941e-01, -3.6871e-01,  1.6573e-01,  ...,  1.4770e+00,\n                          1.5173e+00,  2.3145e+00]],\n              \n                       [[-1.3989e+00, -6.6797e-01,  4.3650e-01,  ..., -1.2556e-01,\n                          3.1578e-02, -1.0537e+00],\n                        [-4.0877e-01,  5.1484e-01,  8.9656e-01,  ...,  7.8346e-02,\n                          1.7703e-01,  4.0171e-02],\n                        [-1.2707e+00, -2.4030e-01,  1.1543e+00,  ...,  8.0477e-01,\n                         -5.9504e-01,  2.6340e-02],\n                        ...,\n                        [-5.9690e-01,  1.0399e-01,  5.4957e-01,  ..., -1.2195e-01,\n                          1.2403e-01,  7.8300e-01],\n                        [ 5.6097e-01,  8.4227e-01,  7.5924e-01,  ..., -3.2427e-02,\n                          1.4369e-01,  9.8922e-02],\n                        [-1.8904e+00, -1.9887e+00, -1.9079e+00,  ..., -5.8492e-02,\n                         -1.7069e-01, -8.0897e-01]]],\n              \n              \n                      [[[ 5.4478e-01,  1.1403e+00,  1.8736e-01,  ...,  1.0092e-01,\n                         -2.3694e-01,  1.4742e-01],\n                        [ 2.9065e-01,  5.0186e-01, -3.7873e-01,  ..., -4.8917e-01,\n                         -4.1328e-01,  1.0882e+00],\n                        [-7.0938e-01,  2.1879e-01, -4.9221e-01,  ...,  7.0891e-01,\n                          1.1955e+00,  1.5179e+00],\n                        ...,\n                        [ 4.0308e+00,  3.3940e+00,  1.1138e+00,  ...,  6.3007e-01,\n                          5.2448e-01,  3.9015e-01],\n                        [ 2.2296e+00,  2.5690e+00,  1.6738e+00,  ...,  3.9876e-01,\n                          1.0844e+00, -9.4805e-01],\n                        [ 1.4075e+00,  9.2620e-01,  1.3191e+00,  ..., -7.7571e-01,\n                         -8.6504e-01, -1.4242e+00]],\n              \n                       [[-6.4149e-01, -1.3346e-01,  8.0991e-02,  ..., -9.7674e-01,\n                         -8.1167e-01, -1.7909e+00],\n                        [-2.8590e-01,  2.2286e-01,  9.0651e-01,  ...,  4.7012e-01,\n                          2.9040e-01, -1.6547e-01],\n                        [-1.4056e+00, -1.2985e+00, -6.9394e-01,  ..., -2.1534e-02,\n                         -1.4241e-01, -4.8687e-01],\n                        ...,\n                        [-3.4934e+00, -2.1168e+00, -6.2417e-01,  ..., -5.8040e-01,\n                          1.7773e+00,  2.7994e-01],\n                        [-3.0342e+00, -2.1509e+00, -8.3051e-01,  ..., -2.7647e+00,\n                         -4.3001e-01,  1.0065e-01],\n                        [-2.6576e+00, -1.4157e+00, -3.4530e-01,  ..., -1.2164e+00,\n                         -2.5716e+00, -7.4760e-01]],\n              \n                       [[-6.2305e-01, -1.6855e-01, -6.1049e-01,  ..., -6.2437e-01,\n                         -9.8096e-01, -4.7883e-01],\n                        [-5.4933e-01,  5.1788e-01, -5.2183e-02,  ...,  1.7339e+00,\n                          1.4311e+00,  1.4968e+00],\n                        [-1.2411e-01,  8.3471e-01,  3.3890e-01,  ...,  1.7176e+00,\n                          1.5044e+00,  1.3674e+00],\n                        ...,\n                        [-7.3788e-01,  1.6625e+00,  5.6045e-01,  ...,  6.5166e-01,\n                          2.2935e-01,  3.6008e-01],\n                        [-3.1418e-01,  2.0752e+00,  2.8765e-01,  ..., -5.8035e-01,\n                         -1.4358e+00, -9.2003e-01],\n                        [ 7.3671e-01,  4.1599e-01, -8.1680e-02,  ...,  6.1118e-01,\n                          6.0759e-01,  4.5051e-01]],\n              \n                       ...,\n              \n                       [[-1.1722e+00, -1.7903e+00, -1.7912e+00,  ..., -1.7509e+00,\n                         -1.3287e+00, -2.0160e+00],\n                        [-2.3188e+00, -1.7166e+00, -2.4675e+00,  ..., -8.5283e-01,\n                         -1.0578e+00, -5.4966e-01],\n                        [-3.0640e-01, -2.4103e-01, -1.2026e+00,  ..., -1.0370e+00,\n                          2.0647e-01, -1.0818e-01],\n                        ...,\n                        [-3.8156e-01,  1.2174e+00,  1.1737e+00,  ...,  1.6563e-01,\n                          1.3626e+00, -1.2316e+00],\n                        [-2.0520e-01,  1.2826e+00, -1.4700e+00,  ..., -3.4698e-01,\n                          3.3692e-01,  4.0425e-01],\n                        [ 8.7686e-01,  4.8647e-01, -2.0410e+00,  ..., -4.2322e-01,\n                          7.4375e-01, -9.6469e-01]],\n              \n                       [[ 5.5214e-01,  1.4008e+00,  1.2890e+00,  ...,  3.7932e-01,\n                          1.6737e+00,  2.0609e+00],\n                        [ 7.6471e-01,  2.1103e+00,  7.8963e-01,  ...,  3.5565e-01,\n                          6.3332e-01,  7.0604e-01],\n                        [ 1.8997e+00,  2.3989e+00,  2.3694e+00,  ...,  3.1218e+00,\n                          2.2509e+00,  1.7461e+00],\n                        ...,\n                        [-3.2860e-01,  1.6003e-01, -1.2622e+00,  ...,  8.0648e-01,\n                          7.2483e-01,  2.1960e-01],\n                        [-1.1304e+00,  1.0691e+00, -5.4193e-01,  ...,  3.0432e-01,\n                         -5.6960e-01,  1.2921e+00],\n                        [-6.5312e-01,  1.5778e+00,  7.2338e-01,  ...,  1.5173e+00,\n                          1.8562e+00,  1.5138e+00]],\n              \n                       [[-1.7487e+00, -1.4121e+00, -4.6035e-01,  ..., -3.0880e-01,\n                          2.1206e-01, -3.8751e-01],\n                        [-8.0148e-01,  3.7642e-02, -9.0254e-01,  ...,  2.0793e-01,\n                         -1.6956e-01, -5.9876e-01],\n                        [-1.8125e+00,  1.0723e-01,  8.7051e-02,  ...,  6.0308e-01,\n                         -1.3145e+00, -2.7071e-01],\n                        ...,\n                        [-9.1168e-01,  9.0865e-01,  1.4953e-02,  ...,  2.8666e+00,\n                          1.5271e+00, -5.3437e-01],\n                        [ 2.1961e-01,  1.9450e+00, -4.6437e-01,  ...,  1.3488e+00,\n                          1.3916e-01,  1.2723e+00],\n                        [-1.3943e+00,  2.0097e-01, -8.4138e-01,  ..., -7.4541e-01,\n                         -3.2763e-01,  1.4120e+00]]]], grad_fn=<AddBackward0>)),\n             (1,\n              tensor([[[[ 6.6338e-03,  2.2980e-01,  5.7714e-01,  ...,  2.7647e-01,\n                          6.2459e-01,  8.6679e-01],\n                        [ 5.4373e-01,  1.8951e+00,  1.4982e+00,  ...,  8.6598e-01,\n                          1.4741e+00,  7.0339e-01],\n                        [ 7.2678e-01,  1.7846e+00,  1.7950e+00,  ...,  6.5961e-01,\n                          5.0298e-01,  7.2746e-01],\n                        ...,\n                        [ 1.4070e-01,  1.6295e+00,  3.5543e-02,  ...,  1.2927e+00,\n                          1.7152e+00,  9.7281e-01],\n                        [ 8.4176e-03,  1.2718e+00, -6.8357e-01,  ...,  1.3568e+00,\n                          7.0412e-01,  9.8973e-01],\n                        [ 2.4377e-01,  3.6193e-01,  6.6175e-01,  ...,  1.1037e+00,\n                          1.0540e+00,  5.3299e-01]],\n              \n                       [[-7.2555e-01,  5.7233e-01, -6.1333e-01,  ..., -9.6135e-01,\n                         -6.6522e-01, -5.7517e-01],\n                        [-1.6551e+00, -2.0330e-02, -3.7474e-01,  ..., -1.9648e+00,\n                         -1.9217e+00, -1.8515e+00],\n                        [-1.1074e+00,  1.3153e-01, -8.0518e-01,  ..., -6.4004e-01,\n                         -1.8804e+00, -1.7779e+00],\n                        ...,\n                        [-2.2620e+00,  5.8329e-01,  6.9189e-01,  ..., -1.1059e+00,\n                         -1.3465e-01, -1.2330e+00],\n                        [-7.9408e-01,  5.0805e-02, -5.4561e-02,  ..., -9.8657e-01,\n                         -1.1258e+00, -8.0736e-01],\n                        [-5.6600e-01, -1.7245e+00,  6.6161e-02,  ..., -8.1438e-01,\n                         -7.1160e-01, -1.1204e+00]],\n              \n                       [[ 2.6836e-01, -1.2983e+00, -4.3233e-02,  ..., -2.4140e-01,\n                         -1.0966e-01,  3.2738e-01],\n                        [-7.8784e-01, -2.7061e+00, -7.0749e-02,  ..., -2.2105e-01,\n                         -1.1780e+00, -5.1780e-01],\n                        [ 2.9280e-01, -8.3232e-01,  7.6183e-01,  ...,  8.5424e-01,\n                          1.6667e-01,  5.8826e-01],\n                        ...,\n                        [-4.6794e-01, -1.0373e-02, -2.3812e+00,  ...,  1.4173e-01,\n                         -2.2146e-01,  3.5997e-01],\n                        [ 1.0082e+00, -4.3210e-01,  8.8013e-01,  ..., -1.7604e-01,\n                          3.3501e-01,  2.5487e-01],\n                        [-9.0739e-01, -2.7295e-01,  4.7976e-01,  ..., -4.0176e-01,\n                         -1.0982e-01, -4.6954e-02]],\n              \n                       ...,\n              \n                       [[-4.2520e-01, -7.7721e-01,  1.6800e-01,  ...,  7.8714e-02,\n                         -1.2970e-01,  3.2282e-01],\n                        [ 6.8645e-01,  3.4730e-01,  3.6207e-01,  ...,  5.2363e-01,\n                          1.2296e+00,  4.8570e-01],\n                        [ 7.8566e-01,  7.2980e-02,  3.1761e-01,  ...,  7.0753e-01,\n                         -8.5860e-01,  3.2976e-01],\n                        ...,\n                        [-4.9694e-01,  7.0742e-02,  1.2741e+00,  ...,  2.8621e-01,\n                         -2.6895e-01, -4.9437e-01],\n                        [-7.1661e-01,  8.0448e-01,  6.3804e-01,  ...,  4.4718e-01,\n                          2.9903e-01, -4.5372e-01],\n                        [-1.0934e+00,  2.0117e-01,  2.7078e-01,  ..., -6.6360e-02,\n                         -2.6611e-01, -3.9825e-01]],\n              \n                       [[ 6.1023e-01,  2.6358e-01,  7.8802e-01,  ...,  7.7292e-01,\n                          2.6831e-01,  4.5780e-01],\n                        [ 7.7873e-01, -4.3179e-01,  5.6368e-02,  ..., -5.7094e-01,\n                          2.4579e-01,  7.6413e-01],\n                        [ 9.0774e-01,  1.4731e-01,  2.0149e-01,  ...,  6.8193e-01,\n                          5.4186e-01,  1.3255e+00],\n                        ...,\n                        [ 2.1024e+00,  2.3125e-01,  7.0855e-01,  ...,  4.5679e-01,\n                          4.6176e-01,  7.2075e-01],\n                        [ 1.3725e+00,  2.7397e-01, -3.6841e-01,  ...,  8.0759e-01,\n                          7.7311e-01,  1.1543e+00],\n                        [ 1.8395e-01,  1.7057e-01, -4.3954e-01,  ...,  6.0174e-01,\n                          1.9542e-01,  6.6372e-01]],\n              \n                       [[ 1.8754e-01,  1.3004e-01,  1.5883e-01,  ..., -6.8792e-02,\n                          3.3370e-01,  4.5566e-01],\n                        [-1.5517e-01,  5.2114e-01,  8.0247e-02,  ..., -9.0456e-02,\n                          5.7119e-01,  8.4730e-02],\n                        [-3.4185e-01, -7.4908e-02, -4.8014e-01,  ..., -2.7701e-01,\n                          3.4473e-02,  4.2675e-01],\n                        ...,\n                        [ 5.7899e-01,  1.0550e+00,  8.0983e-01,  ..., -1.6574e-01,\n                          7.1629e-01, -3.5881e-01],\n                        [-1.7942e-01, -1.4437e+00,  1.0133e+00,  ...,  2.5097e-01,\n                         -4.4543e-04,  1.5746e-01],\n                        [ 1.3421e+00,  1.7975e+00,  1.1472e+00,  ...,  2.1147e-01,\n                          3.7028e-01,  2.5651e-01]]],\n              \n              \n                      [[[-1.0945e-01, -1.4693e-03,  7.6384e-01,  ...,  1.1717e+00,\n                          1.1972e+00,  1.0403e+00],\n                        [ 4.3074e-01,  6.1380e-01,  1.3086e+00,  ...,  1.7727e+00,\n                          1.7882e+00,  1.0919e+00],\n                        [ 2.7506e-01,  7.6879e-01,  1.1789e+00,  ...,  7.1526e-01,\n                          1.4185e+00,  9.8618e-01],\n                        ...,\n                        [ 2.9122e-01, -1.0645e+00,  2.9266e-01,  ...,  1.3561e+00,\n                         -1.1454e-02,  3.2750e-02],\n                        [ 1.3122e-01, -6.9515e-01,  4.7226e-01,  ...,  1.4434e+00,\n                          1.5630e+00,  1.2710e-02],\n                        [ 1.4872e-01,  8.2264e-01,  1.0206e-01,  ...,  1.4330e+00,\n                          1.7425e+00,  4.8475e-01]],\n              \n                       [[-6.7103e-01, -7.5712e-01, -6.3399e-01,  ...,  1.1588e-01,\n                          2.8513e-01, -7.1624e-01],\n                        [-6.5543e-01,  1.8559e-01, -1.1266e+00,  ...,  1.2392e-01,\n                         -5.1139e-01, -1.1380e+00],\n                        [-1.5141e-01, -8.3883e-01, -6.4539e-01,  ...,  9.9135e-01,\n                          2.1401e-01, -1.6540e-01],\n                        ...,\n                        [ 1.3631e-01, -8.2116e-01, -2.2290e-01,  ..., -6.7627e-01,\n                          1.7019e-01, -1.8023e+00],\n                        [-6.3472e-01, -5.1672e-01,  7.7525e-01,  ..., -1.6440e+00,\n                         -5.5320e-02, -1.0787e+00],\n                        [-6.7163e-01, -1.2281e+00, -2.0767e-01,  ..., -1.5072e-01,\n                          3.9507e-01, -1.2938e+00]],\n              \n                       [[ 3.0722e-01, -7.5985e-01, -3.5834e-01,  ..., -3.1578e-02,\n                         -2.4025e-01,  6.6641e-01],\n                        [ 3.8256e-01, -1.6133e+00, -6.4030e-01,  ..., -4.5526e-01,\n                         -3.8613e-01, -1.4542e-01],\n                        [ 5.1435e-01, -1.0546e+00, -3.4131e-01,  ...,  8.6064e-01,\n                          8.4564e-02,  1.4816e-01],\n                        ...,\n                        [ 1.0002e+00, -9.3586e-01, -3.6590e-02,  ...,  5.6953e-01,\n                          1.5977e+00, -5.3115e-01],\n                        [ 1.2589e+00, -3.3720e-01, -5.0333e-01,  ...,  1.1216e+00,\n                          9.2374e-01, -1.5829e-01],\n                        [ 7.7161e-01,  2.9281e-01,  3.9301e-01,  ...,  5.1054e-01,\n                         -5.8571e-01,  3.5652e-01]],\n              \n                       ...,\n              \n                       [[-2.0745e-01, -7.1341e-01,  6.5110e-02,  ..., -6.8676e-01,\n                         -4.4048e-01,  2.2210e-01],\n                        [-4.5601e-02, -2.1526e-01,  9.0794e-02,  ..., -9.5261e-01,\n                         -6.4510e-01, -1.3663e-01],\n                        [-8.8674e-02, -1.3907e-01, -1.1738e+00,  ..., -1.7287e+00,\n                         -8.3006e-01, -6.1073e-01],\n                        ...,\n                        [ 6.5818e-01, -1.0026e+00, -2.5017e-01,  ...,  8.6839e-01,\n                          5.7689e-01,  9.3228e-01],\n                        [ 3.3864e-01,  4.5969e-01, -2.0664e+00,  ..., -1.8808e-01,\n                         -8.5228e-01, -7.9040e-01],\n                        [-1.0316e+00,  9.4493e-01, -1.4285e+00,  ...,  3.6664e-01,\n                         -5.4490e-02, -8.2343e-01]],\n              \n                       [[ 7.0138e-01,  4.1538e-01,  1.0186e+00,  ...,  6.7115e-01,\n                          7.5938e-01,  1.1468e+00],\n                        [ 6.1758e-01, -3.0121e-01, -2.5300e-01,  ...,  2.5732e-01,\n                          8.5292e-01,  1.1765e+00],\n                        [ 1.1211e+00, -7.5055e-01,  5.0837e-01,  ..., -1.2402e-01,\n                          5.9162e-02,  2.8853e-01],\n                        ...,\n                        [-8.2653e-02,  1.0149e+00,  4.3382e-01,  ...,  8.6646e-01,\n                         -3.4067e-01,  4.5112e-01],\n                        [ 3.1154e-01, -1.1062e+00, -9.2806e-01,  ..., -3.5867e-02,\n                         -2.2867e-01, -4.0217e-01],\n                        [ 5.3579e-01,  2.2892e-01, -2.0651e-01,  ...,  5.6216e-01,\n                          7.6869e-01,  1.5483e-01]],\n              \n                       [[ 3.0324e-01,  4.9174e-01,  4.2022e-01,  ...,  4.7927e-01,\n                          4.4197e-01,  6.8507e-01],\n                        [ 6.7470e-01,  1.0700e+00,  7.6937e-01,  ...,  5.2012e-01,\n                          1.3240e+00,  9.3004e-01],\n                        [-5.6201e-01,  3.3050e-01, -1.9552e-02,  ...,  2.1323e-01,\n                          9.8995e-01,  9.7462e-01],\n                        ...,\n                        [-4.3617e-01, -2.0912e-02,  2.6561e-01,  ..., -3.8520e-01,\n                         -5.6162e-01,  5.4239e-01],\n                        [-1.0987e+00, -2.2302e-02, -1.9322e-01,  ...,  3.9891e-01,\n                         -6.9071e-01,  7.6990e-02],\n                        [-9.0886e-01,  6.7768e-01,  5.9165e-01,  ...,  3.1591e-02,\n                          5.5960e-01,  4.5747e-01]]]], grad_fn=<AddBackward0>)),\n             (2,\n              tensor([[[[-8.8086e-02, -3.9787e-01, -2.5728e-01,  ..., -2.6946e-01,\n                         -4.7591e-01, -1.6121e-01],\n                        [ 1.1992e-01, -3.1782e-01, -1.6551e-01,  ..., -5.2750e-01,\n                         -3.9856e-01,  8.3331e-02],\n                        [-1.2674e-02, -4.2653e-01, -3.1078e-01,  ...,  3.6594e-01,\n                         -8.2510e-02, -2.5639e-01],\n                        ...,\n                        [-8.8026e-01, -4.8903e-01, -6.6836e-01,  ...,  4.1641e-01,\n                          9.0186e-01, -8.5776e-01],\n                        [ 1.9495e-02, -6.6365e-01, -1.2705e-01,  ...,  6.5273e-01,\n                          8.4933e-02,  3.9234e-01],\n                        [ 1.5236e-01, -5.0344e-01, -8.0990e-02,  ...,  3.9806e-01,\n                         -4.9124e-01, -3.8858e-01]],\n              \n                       [[-2.1986e-01, -2.9991e-01, -2.8119e-01,  ..., -1.7177e-01,\n                         -4.0530e-01, -6.2434e-01],\n                        [-3.5748e-01, -3.2751e-01, -2.9975e-01,  ..., -4.2092e-01,\n                         -4.9082e-01, -4.5011e-01],\n                        [-6.1478e-01, -5.5291e-01, -4.6776e-01,  ..., -4.5899e-01,\n                         -8.4512e-01, -5.9268e-01],\n                        ...,\n                        [-3.6457e-01, -9.4254e-01, -1.5153e+00,  ..., -1.2877e-01,\n                          7.9194e-01,  3.9247e-01],\n                        [-4.7068e-01, -1.1277e+00, -3.6762e-01,  ...,  3.2445e-02,\n                         -1.8543e-02, -4.7099e-01],\n                        [-4.8964e-01, -8.8155e-01,  3.6315e-02,  ..., -2.6496e-01,\n                         -2.3833e-01, -2.1090e-01]],\n              \n                       [[ 1.7078e-01,  1.4754e-01,  7.6451e-02,  ..., -7.2935e-02,\n                          2.1525e-01,  2.9538e-01],\n                        [ 1.8565e-01,  2.0576e-01, -2.8395e-02,  ..., -1.8767e-01,\n                         -3.1125e-02,  1.4524e-02],\n                        [ 2.8603e-01,  1.6097e-01,  2.6226e-01,  ...,  1.6864e-01,\n                          2.5044e-02,  8.3323e-03],\n                        ...,\n                        [ 2.0063e-01,  1.0619e+00,  1.6648e+00,  ...,  1.0258e+00,\n                          2.5604e+00,  1.7511e+00],\n                        [ 1.2301e-01,  3.1330e-01,  8.6932e-01,  ..., -2.1203e-02,\n                          7.2035e-01,  5.4225e-01],\n                        [ 1.9458e-01, -1.3536e-01, -1.3269e-02,  ..., -8.7379e-02,\n                         -3.6849e-01,  1.2394e-01]],\n              \n                       ...,\n              \n                       [[ 6.7822e-02,  1.8799e-01,  2.6448e-01,  ...,  4.6459e-01,\n                          5.7626e-02,  2.2294e-01],\n                        [-1.1653e-02,  2.9234e-01,  1.2358e-01,  ...,  2.4600e-01,\n                          1.0896e-01,  1.5736e-01],\n                        [-1.7986e-01, -3.7288e-02,  2.3189e-01,  ..., -1.7829e-01,\n                          1.3081e-01,  1.9871e-01],\n                        ...,\n                        [-1.1093e+00,  5.0475e-01, -2.4675e-01,  ..., -1.3060e+00,\n                          4.0962e-01,  1.8121e-01],\n                        [-4.2757e-01, -4.0951e-01,  2.0081e-01,  ..., -2.7452e-01,\n                          1.4840e-01, -6.5696e-02],\n                        [-1.0254e-01,  2.3472e-01,  2.9322e-01,  ..., -1.8366e-01,\n                          7.8709e-02,  2.8473e-01]],\n              \n                       [[ 1.7079e-01,  2.5346e-01,  2.7409e-01,  ...,  6.7073e-02,\n                          1.9207e-02,  4.9547e-01],\n                        [-6.7128e-02,  1.5450e-01, -4.9403e-03,  ..., -2.2646e-01,\n                          5.6951e-02,  2.7930e-01],\n                        [-5.2665e-02, -8.2181e-02, -9.2547e-02,  ..., -1.4519e-01,\n                         -9.0544e-02,  2.4041e-01],\n                        ...,\n                        [ 4.1616e-01,  2.0366e-01, -2.6858e-01,  ..., -7.5477e-01,\n                          4.5500e-01,  9.7671e-01],\n                        [ 9.7483e-02,  4.4645e-01,  8.4723e-02,  ..., -2.1907e-01,\n                         -1.3717e-01,  2.6413e-01],\n                        [ 1.8776e-01,  6.9736e-03, -5.5637e-01,  ..., -2.8416e-02,\n                          2.5515e-01,  4.6580e-01]],\n              \n                       [[ 8.1063e-03, -1.7806e-02, -3.1848e-01,  ...,  1.4365e-01,\n                         -3.7737e-02,  1.2483e-01],\n                        [-8.5387e-02,  1.6120e-01, -2.7067e-01,  ..., -2.4433e-01,\n                         -9.3544e-02, -1.0003e-01],\n                        [-3.2188e-01, -3.6161e-01, -3.3112e-01,  ..., -6.0482e-01,\n                         -3.0834e-01,  2.4398e-02],\n                        ...,\n                        [ 1.2053e-01, -3.9799e-01,  1.0407e-01,  ...,  1.5394e+00,\n                          4.5902e-01, -9.6906e-01],\n                        [-7.4711e-01, -1.7102e-01, -8.2709e-01,  ..., -4.9746e-01,\n                         -1.2640e-01,  4.3773e-02],\n                        [-2.2919e-01, -1.2267e-01, -7.6370e-01,  ..., -3.8480e-01,\n                          7.0970e-02,  2.4827e-01]]],\n              \n              \n                      [[[-4.8104e-02, -3.9557e-01, -2.7095e-01,  ..., -3.4844e-01,\n                         -2.3906e-01, -2.5550e-01],\n                        [-1.9117e-02, -1.1523e-01, -1.6701e-01,  ...,  1.0418e-01,\n                         -1.3509e-01, -1.6501e-01],\n                        [-4.3906e-02, -7.0913e-02, -1.5929e-01,  ..., -7.6162e-02,\n                          1.3345e-02, -4.6048e-02],\n                        ...,\n                        [ 3.1323e-01,  4.0539e-02, -1.3182e-01,  ...,  2.3887e-01,\n                         -3.0051e-01, -3.9242e-02],\n                        [ 2.2863e-03,  1.0602e-01, -1.9749e-01,  ...,  3.4189e-01,\n                         -1.8437e-01, -4.1512e-02],\n                        [ 1.4937e-01,  4.2384e-01,  1.4425e-01,  ...,  1.6882e-01,\n                          1.9218e-01, -3.8202e-02]],\n              \n                       [[-9.2120e-02, -2.4386e-01, -2.3827e-01,  ...,  1.0953e-01,\n                         -4.4733e-01, -6.8849e-01],\n                        [-4.1938e-01, -5.1203e-01, -5.5262e-01,  ..., -3.6812e-01,\n                         -7.4945e-01, -7.8304e-01],\n                        [-5.3313e-01, -4.2840e-01, -6.3894e-01,  ..., -1.6120e-01,\n                         -4.2165e-01, -6.8779e-01],\n                        ...,\n                        [-6.3385e-01, -3.9571e-01, -5.1120e-01,  ...,  7.5354e-01,\n                         -8.9978e-01, -6.8837e-01],\n                        [-3.6109e-01, -2.2821e-01, -1.8437e-01,  ..., -4.8366e-01,\n                          5.1462e-02, -3.0329e-01],\n                        [-4.1273e-01, -4.4439e-01, -4.4651e-01,  ..., -7.4278e-01,\n                         -3.2459e-01, -3.7495e-01]],\n              \n                       [[ 1.1926e-01,  9.5121e-02,  1.5751e-01,  ..., -1.3779e-02,\n                          7.0230e-02,  2.0632e-01],\n                        [ 2.6625e-01,  5.1537e-02,  9.5987e-02,  ..., -4.4795e-01,\n                          1.8810e-01,  2.2377e-01],\n                        [ 5.1145e-01,  1.8852e-01,  2.0161e-01,  ..., -1.9842e-01,\n                          1.2217e-02,  2.7290e-01],\n                        ...,\n                        [ 3.3658e-01,  3.6763e-01,  1.3941e-01,  ..., -4.4949e-02,\n                          3.4857e-01, -4.3137e-02],\n                        [ 4.0758e-01,  6.4796e-02,  2.4149e-01,  ...,  3.0350e-02,\n                          3.4041e-01,  4.0712e-01],\n                        [ 4.5716e-01,  3.0545e-01,  3.6638e-01,  ...,  1.5763e-01,\n                          3.4303e-01,  1.5706e-01]],\n              \n                       ...,\n              \n                       [[-6.7573e-02,  8.3438e-02,  2.4324e-01,  ...,  1.4158e-02,\n                         -2.5162e-01,  1.6434e-01],\n                        [-2.4061e-02,  1.7137e-01,  1.0950e-01,  ...,  3.2367e-01,\n                         -3.1223e-01,  1.3555e-01],\n                        [-1.9832e-01,  5.0264e-02,  3.5868e-02,  ...,  1.3552e-01,\n                          1.2958e-01,  2.0913e-01],\n                        ...,\n                        [-4.6597e-03,  4.9366e-02,  1.8714e-01,  ...,  1.0177e+00,\n                         -1.6098e-01,  2.3058e-01],\n                        [ 7.5481e-02,  1.4444e-01,  2.8364e-02,  ...,  1.5517e-01,\n                         -1.2283e-01,  3.7836e-02],\n                        [ 1.6428e-01,  3.0505e-01,  1.3647e-01,  ..., -2.8294e-01,\n                          5.5076e-02, -5.7954e-02]],\n              \n                       [[-1.0970e-01,  1.8579e-01,  4.2916e-02,  ..., -1.3882e-01,\n                          5.9157e-02,  1.9676e-01],\n                        [-8.3217e-02, -1.1038e-01, -2.5567e-01,  ..., -6.0023e-01,\n                         -3.7114e-01,  3.0515e-02],\n                        [-1.2109e-01, -9.9213e-02, -2.0169e-01,  ..., -4.2881e-01,\n                         -4.3398e-01, -1.2138e-01],\n                        ...,\n                        [-1.5314e-02,  2.7062e-01, -1.8329e-01,  ..., -3.9080e-01,\n                         -2.5083e-01,  2.2430e-01],\n                        [ 2.1567e-01, -3.0287e-02, -2.9929e-01,  ..., -8.5693e-01,\n                         -2.4367e-01,  7.8193e-02],\n                        [ 2.5349e-01,  1.1735e-02, -4.9794e-02,  ..., -7.9913e-01,\n                         -1.6857e-01,  2.0306e-01]],\n              \n                       [[ 1.3910e-02, -1.4393e-01, -2.4672e-01,  ..., -2.9001e-01,\n                         -1.5070e-01,  1.5505e-01],\n                        [ 9.7321e-02,  9.0710e-02, -3.9046e-01,  ..., -3.6714e-01,\n                         -1.0289e-01, -1.8730e-02],\n                        [-3.6524e-01, -3.3544e-01, -4.1749e-01,  ...,  2.3299e-01,\n                          1.9160e-01,  6.7667e-02],\n                        ...,\n                        [-3.7399e-02,  9.4682e-02, -4.0049e-01,  ..., -5.1874e-01,\n                         -3.3716e-01, -1.5224e-02],\n                        [-1.4139e-01, -2.3622e-01, -1.4169e-01,  ..., -1.7983e-02,\n                         -1.0865e-02, -8.3069e-03],\n                        [ 2.0323e-01,  3.0407e-01, -2.8859e-02,  ..., -1.5721e+00,\n                          4.3957e-01, -1.3377e-01]]]], grad_fn=<AddBackward0>))])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs = body5(images)\n",
    "body5outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys([0, 1, 2])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_values([tensor([[[[ 1.1721e+00,  6.9228e-02, -3.9747e-01,  ...,  8.7818e-01,\n           -1.1011e-01,  2.8548e-01],\n          [ 4.7327e-01, -1.0635e+00,  2.7001e-02,  ..., -8.0862e-01,\n           -7.9550e-01, -1.6185e-01],\n          [ 1.1909e+00, -2.7725e-01,  6.9662e-01,  ...,  5.5011e-01,\n            1.0003e+00,  6.8787e-01],\n          ...,\n          [-4.8151e-01,  5.1347e-02, -1.1363e+00,  ...,  1.2122e-03,\n           -4.8677e-01, -2.3148e-01],\n          [ 9.6476e-01,  6.3809e-01,  1.6118e+00,  ..., -4.2627e-02,\n           -9.9582e-02,  4.4157e-01],\n          [ 9.6395e-01,  1.5906e+00,  1.0402e+00,  ..., -7.9937e-01,\n           -8.5396e-01,  1.9993e-01]],\n\n         [[-8.7627e-01, -5.4330e-01, -2.5792e-02,  ..., -1.8612e-01,\n           -6.4415e-02, -1.1243e+00],\n          [-2.5136e-01, -4.8535e-01,  5.4443e-01,  ...,  5.4259e-01,\n            7.8116e-01,  3.4117e-01],\n          [-1.4178e+00, -8.5801e-01,  1.0186e+00,  ..., -5.5463e-02,\n            9.2877e-02,  1.1118e+00],\n          ...,\n          [ 2.6230e-01,  1.4150e+00, -1.9990e-01,  ..., -2.0833e-02,\n           -5.9561e-01, -1.0818e-01],\n          [-5.4785e-01, -1.7448e+00,  1.5543e+00,  ..., -7.2155e-01,\n           -1.2494e+00, -8.2264e-01],\n          [-9.2908e-01, -1.5372e+00, -3.4183e+00,  ..., -3.8194e-01,\n           -4.8590e-01, -9.9215e-01]],\n\n         [[-2.0940e-01, -1.0195e+00, -1.0586e+00,  ..., -4.5160e-01,\n            1.9782e-01,  1.2470e-02],\n          [ 2.5020e-01,  9.4780e-01,  1.3655e+00,  ...,  6.8982e-01,\n            3.3426e-01,  1.2425e+00],\n          [-8.9640e-02,  1.1651e-01,  2.4986e-01,  ...,  1.4733e-01,\n            3.0400e-01,  2.6642e-01],\n          ...,\n          [-5.8508e-01,  6.6090e-01, -2.3515e-01,  ...,  2.5706e-01,\n            3.0195e-01, -4.8154e-01],\n          [-6.8679e-01, -9.0359e-01, -8.4630e-01,  ...,  2.0000e-01,\n           -9.1194e-02,  9.0681e-02],\n          [ 4.9514e-01,  1.6723e+00,  1.9171e+00,  ...,  5.0900e-02,\n            1.9158e-01,  7.0022e-02]],\n\n         ...,\n\n         [[-2.1618e+00, -1.1193e+00, -2.7137e+00,  ..., -2.0935e+00,\n           -7.5401e-01, -6.3502e-01],\n          [-2.3586e+00, -1.1661e+00, -1.8013e+00,  ..., -9.4699e-01,\n           -1.2291e+00, -8.1769e-01],\n          [-1.0303e+00, -2.2719e-01,  4.1982e-01,  ...,  4.9353e-01,\n            5.3990e-01,  5.7499e-01],\n          ...,\n          [-1.4189e+00,  1.4990e+00,  1.1591e+00,  ...,  1.3413e-01,\n           -1.3431e-01,  3.2896e-01],\n          [-9.6739e-01,  1.0597e+00,  3.5422e+00,  ..., -4.2319e-01,\n           -9.3573e-01, -5.7952e-01],\n          [-1.4164e-01,  1.3100e+00,  2.5119e+00,  ...,  2.7821e-01,\n            4.6052e-01, -5.4587e-01]],\n\n         [[ 5.6649e-01,  1.1662e+00,  9.2817e-01,  ...,  1.0175e+00,\n            9.0750e-01,  1.7660e+00],\n          [ 3.6170e-01,  9.5654e-01, -5.1820e-01,  ..., -5.4050e-01,\n            1.8852e-02,  7.1536e-01],\n          [ 1.1596e+00,  5.7209e-01,  1.8089e+00,  ...,  1.3506e+00,\n            1.4309e+00,  8.2315e-01],\n          ...,\n          [ 1.3010e+00,  4.5754e-01,  5.6102e-01,  ...,  6.5244e-01,\n            7.9862e-01,  1.2213e+00],\n          [-3.1050e-01,  2.8720e-01, -4.0178e-01,  ...,  4.0922e-01,\n            7.9655e-01,  1.6221e+00],\n          [ 4.0941e-01, -3.6871e-01,  1.6573e-01,  ...,  1.4770e+00,\n            1.5173e+00,  2.3145e+00]],\n\n         [[-1.3989e+00, -6.6797e-01,  4.3650e-01,  ..., -1.2556e-01,\n            3.1578e-02, -1.0537e+00],\n          [-4.0877e-01,  5.1484e-01,  8.9656e-01,  ...,  7.8346e-02,\n            1.7703e-01,  4.0171e-02],\n          [-1.2707e+00, -2.4030e-01,  1.1543e+00,  ...,  8.0477e-01,\n           -5.9504e-01,  2.6340e-02],\n          ...,\n          [-5.9690e-01,  1.0399e-01,  5.4957e-01,  ..., -1.2195e-01,\n            1.2403e-01,  7.8300e-01],\n          [ 5.6097e-01,  8.4227e-01,  7.5924e-01,  ..., -3.2427e-02,\n            1.4369e-01,  9.8922e-02],\n          [-1.8904e+00, -1.9887e+00, -1.9079e+00,  ..., -5.8492e-02,\n           -1.7069e-01, -8.0897e-01]]],\n\n\n        [[[ 5.4478e-01,  1.1403e+00,  1.8736e-01,  ...,  1.0092e-01,\n           -2.3694e-01,  1.4742e-01],\n          [ 2.9065e-01,  5.0186e-01, -3.7873e-01,  ..., -4.8917e-01,\n           -4.1328e-01,  1.0882e+00],\n          [-7.0938e-01,  2.1879e-01, -4.9221e-01,  ...,  7.0891e-01,\n            1.1955e+00,  1.5179e+00],\n          ...,\n          [ 4.0308e+00,  3.3940e+00,  1.1138e+00,  ...,  6.3007e-01,\n            5.2448e-01,  3.9015e-01],\n          [ 2.2296e+00,  2.5690e+00,  1.6738e+00,  ...,  3.9876e-01,\n            1.0844e+00, -9.4805e-01],\n          [ 1.4075e+00,  9.2620e-01,  1.3191e+00,  ..., -7.7571e-01,\n           -8.6504e-01, -1.4242e+00]],\n\n         [[-6.4149e-01, -1.3346e-01,  8.0991e-02,  ..., -9.7674e-01,\n           -8.1167e-01, -1.7909e+00],\n          [-2.8590e-01,  2.2286e-01,  9.0651e-01,  ...,  4.7012e-01,\n            2.9040e-01, -1.6547e-01],\n          [-1.4056e+00, -1.2985e+00, -6.9394e-01,  ..., -2.1534e-02,\n           -1.4241e-01, -4.8687e-01],\n          ...,\n          [-3.4934e+00, -2.1168e+00, -6.2417e-01,  ..., -5.8040e-01,\n            1.7773e+00,  2.7994e-01],\n          [-3.0342e+00, -2.1509e+00, -8.3051e-01,  ..., -2.7647e+00,\n           -4.3001e-01,  1.0065e-01],\n          [-2.6576e+00, -1.4157e+00, -3.4530e-01,  ..., -1.2164e+00,\n           -2.5716e+00, -7.4760e-01]],\n\n         [[-6.2305e-01, -1.6855e-01, -6.1049e-01,  ..., -6.2437e-01,\n           -9.8096e-01, -4.7883e-01],\n          [-5.4933e-01,  5.1788e-01, -5.2183e-02,  ...,  1.7339e+00,\n            1.4311e+00,  1.4968e+00],\n          [-1.2411e-01,  8.3471e-01,  3.3890e-01,  ...,  1.7176e+00,\n            1.5044e+00,  1.3674e+00],\n          ...,\n          [-7.3788e-01,  1.6625e+00,  5.6045e-01,  ...,  6.5166e-01,\n            2.2935e-01,  3.6008e-01],\n          [-3.1418e-01,  2.0752e+00,  2.8765e-01,  ..., -5.8035e-01,\n           -1.4358e+00, -9.2003e-01],\n          [ 7.3671e-01,  4.1599e-01, -8.1680e-02,  ...,  6.1118e-01,\n            6.0759e-01,  4.5051e-01]],\n\n         ...,\n\n         [[-1.1722e+00, -1.7903e+00, -1.7912e+00,  ..., -1.7509e+00,\n           -1.3287e+00, -2.0160e+00],\n          [-2.3188e+00, -1.7166e+00, -2.4675e+00,  ..., -8.5283e-01,\n           -1.0578e+00, -5.4966e-01],\n          [-3.0640e-01, -2.4103e-01, -1.2026e+00,  ..., -1.0370e+00,\n            2.0647e-01, -1.0818e-01],\n          ...,\n          [-3.8156e-01,  1.2174e+00,  1.1737e+00,  ...,  1.6563e-01,\n            1.3626e+00, -1.2316e+00],\n          [-2.0520e-01,  1.2826e+00, -1.4700e+00,  ..., -3.4698e-01,\n            3.3692e-01,  4.0425e-01],\n          [ 8.7686e-01,  4.8647e-01, -2.0410e+00,  ..., -4.2322e-01,\n            7.4375e-01, -9.6469e-01]],\n\n         [[ 5.5214e-01,  1.4008e+00,  1.2890e+00,  ...,  3.7932e-01,\n            1.6737e+00,  2.0609e+00],\n          [ 7.6471e-01,  2.1103e+00,  7.8963e-01,  ...,  3.5565e-01,\n            6.3332e-01,  7.0604e-01],\n          [ 1.8997e+00,  2.3989e+00,  2.3694e+00,  ...,  3.1218e+00,\n            2.2509e+00,  1.7461e+00],\n          ...,\n          [-3.2860e-01,  1.6003e-01, -1.2622e+00,  ...,  8.0648e-01,\n            7.2483e-01,  2.1960e-01],\n          [-1.1304e+00,  1.0691e+00, -5.4193e-01,  ...,  3.0432e-01,\n           -5.6960e-01,  1.2921e+00],\n          [-6.5312e-01,  1.5778e+00,  7.2338e-01,  ...,  1.5173e+00,\n            1.8562e+00,  1.5138e+00]],\n\n         [[-1.7487e+00, -1.4121e+00, -4.6035e-01,  ..., -3.0880e-01,\n            2.1206e-01, -3.8751e-01],\n          [-8.0148e-01,  3.7642e-02, -9.0254e-01,  ...,  2.0793e-01,\n           -1.6956e-01, -5.9876e-01],\n          [-1.8125e+00,  1.0723e-01,  8.7051e-02,  ...,  6.0308e-01,\n           -1.3145e+00, -2.7071e-01],\n          ...,\n          [-9.1168e-01,  9.0865e-01,  1.4953e-02,  ...,  2.8666e+00,\n            1.5271e+00, -5.3437e-01],\n          [ 2.1961e-01,  1.9450e+00, -4.6437e-01,  ...,  1.3488e+00,\n            1.3916e-01,  1.2723e+00],\n          [-1.3943e+00,  2.0097e-01, -8.4138e-01,  ..., -7.4541e-01,\n           -3.2763e-01,  1.4120e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 6.6338e-03,  2.2980e-01,  5.7714e-01,  ...,  2.7647e-01,\n            6.2459e-01,  8.6679e-01],\n          [ 5.4373e-01,  1.8951e+00,  1.4982e+00,  ...,  8.6598e-01,\n            1.4741e+00,  7.0339e-01],\n          [ 7.2678e-01,  1.7846e+00,  1.7950e+00,  ...,  6.5961e-01,\n            5.0298e-01,  7.2746e-01],\n          ...,\n          [ 1.4070e-01,  1.6295e+00,  3.5543e-02,  ...,  1.2927e+00,\n            1.7152e+00,  9.7281e-01],\n          [ 8.4176e-03,  1.2718e+00, -6.8357e-01,  ...,  1.3568e+00,\n            7.0412e-01,  9.8973e-01],\n          [ 2.4377e-01,  3.6193e-01,  6.6175e-01,  ...,  1.1037e+00,\n            1.0540e+00,  5.3299e-01]],\n\n         [[-7.2555e-01,  5.7233e-01, -6.1333e-01,  ..., -9.6135e-01,\n           -6.6522e-01, -5.7517e-01],\n          [-1.6551e+00, -2.0330e-02, -3.7474e-01,  ..., -1.9648e+00,\n           -1.9217e+00, -1.8515e+00],\n          [-1.1074e+00,  1.3153e-01, -8.0518e-01,  ..., -6.4004e-01,\n           -1.8804e+00, -1.7779e+00],\n          ...,\n          [-2.2620e+00,  5.8329e-01,  6.9189e-01,  ..., -1.1059e+00,\n           -1.3465e-01, -1.2330e+00],\n          [-7.9408e-01,  5.0805e-02, -5.4561e-02,  ..., -9.8657e-01,\n           -1.1258e+00, -8.0736e-01],\n          [-5.6600e-01, -1.7245e+00,  6.6161e-02,  ..., -8.1438e-01,\n           -7.1160e-01, -1.1204e+00]],\n\n         [[ 2.6836e-01, -1.2983e+00, -4.3233e-02,  ..., -2.4140e-01,\n           -1.0966e-01,  3.2738e-01],\n          [-7.8784e-01, -2.7061e+00, -7.0749e-02,  ..., -2.2105e-01,\n           -1.1780e+00, -5.1780e-01],\n          [ 2.9280e-01, -8.3232e-01,  7.6183e-01,  ...,  8.5424e-01,\n            1.6667e-01,  5.8826e-01],\n          ...,\n          [-4.6794e-01, -1.0373e-02, -2.3812e+00,  ...,  1.4173e-01,\n           -2.2146e-01,  3.5997e-01],\n          [ 1.0082e+00, -4.3210e-01,  8.8013e-01,  ..., -1.7604e-01,\n            3.3501e-01,  2.5487e-01],\n          [-9.0739e-01, -2.7295e-01,  4.7976e-01,  ..., -4.0176e-01,\n           -1.0982e-01, -4.6954e-02]],\n\n         ...,\n\n         [[-4.2520e-01, -7.7721e-01,  1.6800e-01,  ...,  7.8714e-02,\n           -1.2970e-01,  3.2282e-01],\n          [ 6.8645e-01,  3.4730e-01,  3.6207e-01,  ...,  5.2363e-01,\n            1.2296e+00,  4.8570e-01],\n          [ 7.8566e-01,  7.2980e-02,  3.1761e-01,  ...,  7.0753e-01,\n           -8.5860e-01,  3.2976e-01],\n          ...,\n          [-4.9694e-01,  7.0742e-02,  1.2741e+00,  ...,  2.8621e-01,\n           -2.6895e-01, -4.9437e-01],\n          [-7.1661e-01,  8.0448e-01,  6.3804e-01,  ...,  4.4718e-01,\n            2.9903e-01, -4.5372e-01],\n          [-1.0934e+00,  2.0117e-01,  2.7078e-01,  ..., -6.6360e-02,\n           -2.6611e-01, -3.9825e-01]],\n\n         [[ 6.1023e-01,  2.6358e-01,  7.8802e-01,  ...,  7.7292e-01,\n            2.6831e-01,  4.5780e-01],\n          [ 7.7873e-01, -4.3179e-01,  5.6368e-02,  ..., -5.7094e-01,\n            2.4579e-01,  7.6413e-01],\n          [ 9.0774e-01,  1.4731e-01,  2.0149e-01,  ...,  6.8193e-01,\n            5.4186e-01,  1.3255e+00],\n          ...,\n          [ 2.1024e+00,  2.3125e-01,  7.0855e-01,  ...,  4.5679e-01,\n            4.6176e-01,  7.2075e-01],\n          [ 1.3725e+00,  2.7397e-01, -3.6841e-01,  ...,  8.0759e-01,\n            7.7311e-01,  1.1543e+00],\n          [ 1.8395e-01,  1.7057e-01, -4.3954e-01,  ...,  6.0174e-01,\n            1.9542e-01,  6.6372e-01]],\n\n         [[ 1.8754e-01,  1.3004e-01,  1.5883e-01,  ..., -6.8792e-02,\n            3.3370e-01,  4.5566e-01],\n          [-1.5517e-01,  5.2114e-01,  8.0247e-02,  ..., -9.0456e-02,\n            5.7119e-01,  8.4730e-02],\n          [-3.4185e-01, -7.4908e-02, -4.8014e-01,  ..., -2.7701e-01,\n            3.4473e-02,  4.2675e-01],\n          ...,\n          [ 5.7899e-01,  1.0550e+00,  8.0983e-01,  ..., -1.6574e-01,\n            7.1629e-01, -3.5881e-01],\n          [-1.7942e-01, -1.4437e+00,  1.0133e+00,  ...,  2.5097e-01,\n           -4.4543e-04,  1.5746e-01],\n          [ 1.3421e+00,  1.7975e+00,  1.1472e+00,  ...,  2.1147e-01,\n            3.7028e-01,  2.5651e-01]]],\n\n\n        [[[-1.0945e-01, -1.4693e-03,  7.6384e-01,  ...,  1.1717e+00,\n            1.1972e+00,  1.0403e+00],\n          [ 4.3074e-01,  6.1380e-01,  1.3086e+00,  ...,  1.7727e+00,\n            1.7882e+00,  1.0919e+00],\n          [ 2.7506e-01,  7.6879e-01,  1.1789e+00,  ...,  7.1526e-01,\n            1.4185e+00,  9.8618e-01],\n          ...,\n          [ 2.9122e-01, -1.0645e+00,  2.9266e-01,  ...,  1.3561e+00,\n           -1.1454e-02,  3.2750e-02],\n          [ 1.3122e-01, -6.9515e-01,  4.7226e-01,  ...,  1.4434e+00,\n            1.5630e+00,  1.2710e-02],\n          [ 1.4872e-01,  8.2264e-01,  1.0206e-01,  ...,  1.4330e+00,\n            1.7425e+00,  4.8475e-01]],\n\n         [[-6.7103e-01, -7.5712e-01, -6.3399e-01,  ...,  1.1588e-01,\n            2.8513e-01, -7.1624e-01],\n          [-6.5543e-01,  1.8559e-01, -1.1266e+00,  ...,  1.2392e-01,\n           -5.1139e-01, -1.1380e+00],\n          [-1.5141e-01, -8.3883e-01, -6.4539e-01,  ...,  9.9135e-01,\n            2.1401e-01, -1.6540e-01],\n          ...,\n          [ 1.3631e-01, -8.2116e-01, -2.2290e-01,  ..., -6.7627e-01,\n            1.7019e-01, -1.8023e+00],\n          [-6.3472e-01, -5.1672e-01,  7.7525e-01,  ..., -1.6440e+00,\n           -5.5320e-02, -1.0787e+00],\n          [-6.7163e-01, -1.2281e+00, -2.0767e-01,  ..., -1.5072e-01,\n            3.9507e-01, -1.2938e+00]],\n\n         [[ 3.0722e-01, -7.5985e-01, -3.5834e-01,  ..., -3.1578e-02,\n           -2.4025e-01,  6.6641e-01],\n          [ 3.8256e-01, -1.6133e+00, -6.4030e-01,  ..., -4.5526e-01,\n           -3.8613e-01, -1.4542e-01],\n          [ 5.1435e-01, -1.0546e+00, -3.4131e-01,  ...,  8.6064e-01,\n            8.4564e-02,  1.4816e-01],\n          ...,\n          [ 1.0002e+00, -9.3586e-01, -3.6590e-02,  ...,  5.6953e-01,\n            1.5977e+00, -5.3115e-01],\n          [ 1.2589e+00, -3.3720e-01, -5.0333e-01,  ...,  1.1216e+00,\n            9.2374e-01, -1.5829e-01],\n          [ 7.7161e-01,  2.9281e-01,  3.9301e-01,  ...,  5.1054e-01,\n           -5.8571e-01,  3.5652e-01]],\n\n         ...,\n\n         [[-2.0745e-01, -7.1341e-01,  6.5110e-02,  ..., -6.8676e-01,\n           -4.4048e-01,  2.2210e-01],\n          [-4.5601e-02, -2.1526e-01,  9.0794e-02,  ..., -9.5261e-01,\n           -6.4510e-01, -1.3663e-01],\n          [-8.8674e-02, -1.3907e-01, -1.1738e+00,  ..., -1.7287e+00,\n           -8.3006e-01, -6.1073e-01],\n          ...,\n          [ 6.5818e-01, -1.0026e+00, -2.5017e-01,  ...,  8.6839e-01,\n            5.7689e-01,  9.3228e-01],\n          [ 3.3864e-01,  4.5969e-01, -2.0664e+00,  ..., -1.8808e-01,\n           -8.5228e-01, -7.9040e-01],\n          [-1.0316e+00,  9.4493e-01, -1.4285e+00,  ...,  3.6664e-01,\n           -5.4490e-02, -8.2343e-01]],\n\n         [[ 7.0138e-01,  4.1538e-01,  1.0186e+00,  ...,  6.7115e-01,\n            7.5938e-01,  1.1468e+00],\n          [ 6.1758e-01, -3.0121e-01, -2.5300e-01,  ...,  2.5732e-01,\n            8.5292e-01,  1.1765e+00],\n          [ 1.1211e+00, -7.5055e-01,  5.0837e-01,  ..., -1.2402e-01,\n            5.9162e-02,  2.8853e-01],\n          ...,\n          [-8.2653e-02,  1.0149e+00,  4.3382e-01,  ...,  8.6646e-01,\n           -3.4067e-01,  4.5112e-01],\n          [ 3.1154e-01, -1.1062e+00, -9.2806e-01,  ..., -3.5867e-02,\n           -2.2867e-01, -4.0217e-01],\n          [ 5.3579e-01,  2.2892e-01, -2.0651e-01,  ...,  5.6216e-01,\n            7.6869e-01,  1.5483e-01]],\n\n         [[ 3.0324e-01,  4.9174e-01,  4.2022e-01,  ...,  4.7927e-01,\n            4.4197e-01,  6.8507e-01],\n          [ 6.7470e-01,  1.0700e+00,  7.6937e-01,  ...,  5.2012e-01,\n            1.3240e+00,  9.3004e-01],\n          [-5.6201e-01,  3.3050e-01, -1.9552e-02,  ...,  2.1323e-01,\n            9.8995e-01,  9.7462e-01],\n          ...,\n          [-4.3617e-01, -2.0912e-02,  2.6561e-01,  ..., -3.8520e-01,\n           -5.6162e-01,  5.4239e-01],\n          [-1.0987e+00, -2.2302e-02, -1.9322e-01,  ...,  3.9891e-01,\n           -6.9071e-01,  7.6990e-02],\n          [-9.0886e-01,  6.7768e-01,  5.9165e-01,  ...,  3.1591e-02,\n            5.5960e-01,  4.5747e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-8.8086e-02, -3.9787e-01, -2.5728e-01,  ..., -2.6946e-01,\n           -4.7591e-01, -1.6121e-01],\n          [ 1.1992e-01, -3.1782e-01, -1.6551e-01,  ..., -5.2750e-01,\n           -3.9856e-01,  8.3331e-02],\n          [-1.2674e-02, -4.2653e-01, -3.1078e-01,  ...,  3.6594e-01,\n           -8.2510e-02, -2.5639e-01],\n          ...,\n          [-8.8026e-01, -4.8903e-01, -6.6836e-01,  ...,  4.1641e-01,\n            9.0186e-01, -8.5776e-01],\n          [ 1.9495e-02, -6.6365e-01, -1.2705e-01,  ...,  6.5273e-01,\n            8.4933e-02,  3.9234e-01],\n          [ 1.5236e-01, -5.0344e-01, -8.0990e-02,  ...,  3.9806e-01,\n           -4.9124e-01, -3.8858e-01]],\n\n         [[-2.1986e-01, -2.9991e-01, -2.8119e-01,  ..., -1.7177e-01,\n           -4.0530e-01, -6.2434e-01],\n          [-3.5748e-01, -3.2751e-01, -2.9975e-01,  ..., -4.2092e-01,\n           -4.9082e-01, -4.5011e-01],\n          [-6.1478e-01, -5.5291e-01, -4.6776e-01,  ..., -4.5899e-01,\n           -8.4512e-01, -5.9268e-01],\n          ...,\n          [-3.6457e-01, -9.4254e-01, -1.5153e+00,  ..., -1.2877e-01,\n            7.9194e-01,  3.9247e-01],\n          [-4.7068e-01, -1.1277e+00, -3.6762e-01,  ...,  3.2445e-02,\n           -1.8543e-02, -4.7099e-01],\n          [-4.8964e-01, -8.8155e-01,  3.6315e-02,  ..., -2.6496e-01,\n           -2.3833e-01, -2.1090e-01]],\n\n         [[ 1.7078e-01,  1.4754e-01,  7.6451e-02,  ..., -7.2935e-02,\n            2.1525e-01,  2.9538e-01],\n          [ 1.8565e-01,  2.0576e-01, -2.8395e-02,  ..., -1.8767e-01,\n           -3.1125e-02,  1.4524e-02],\n          [ 2.8603e-01,  1.6097e-01,  2.6226e-01,  ...,  1.6864e-01,\n            2.5044e-02,  8.3323e-03],\n          ...,\n          [ 2.0063e-01,  1.0619e+00,  1.6648e+00,  ...,  1.0258e+00,\n            2.5604e+00,  1.7511e+00],\n          [ 1.2301e-01,  3.1330e-01,  8.6932e-01,  ..., -2.1203e-02,\n            7.2035e-01,  5.4225e-01],\n          [ 1.9458e-01, -1.3536e-01, -1.3269e-02,  ..., -8.7379e-02,\n           -3.6849e-01,  1.2394e-01]],\n\n         ...,\n\n         [[ 6.7822e-02,  1.8799e-01,  2.6448e-01,  ...,  4.6459e-01,\n            5.7626e-02,  2.2294e-01],\n          [-1.1653e-02,  2.9234e-01,  1.2358e-01,  ...,  2.4600e-01,\n            1.0896e-01,  1.5736e-01],\n          [-1.7986e-01, -3.7288e-02,  2.3189e-01,  ..., -1.7829e-01,\n            1.3081e-01,  1.9871e-01],\n          ...,\n          [-1.1093e+00,  5.0475e-01, -2.4675e-01,  ..., -1.3060e+00,\n            4.0962e-01,  1.8121e-01],\n          [-4.2757e-01, -4.0951e-01,  2.0081e-01,  ..., -2.7452e-01,\n            1.4840e-01, -6.5696e-02],\n          [-1.0254e-01,  2.3472e-01,  2.9322e-01,  ..., -1.8366e-01,\n            7.8709e-02,  2.8473e-01]],\n\n         [[ 1.7079e-01,  2.5346e-01,  2.7409e-01,  ...,  6.7073e-02,\n            1.9207e-02,  4.9547e-01],\n          [-6.7128e-02,  1.5450e-01, -4.9403e-03,  ..., -2.2646e-01,\n            5.6951e-02,  2.7930e-01],\n          [-5.2665e-02, -8.2181e-02, -9.2547e-02,  ..., -1.4519e-01,\n           -9.0544e-02,  2.4041e-01],\n          ...,\n          [ 4.1616e-01,  2.0366e-01, -2.6858e-01,  ..., -7.5477e-01,\n            4.5500e-01,  9.7671e-01],\n          [ 9.7483e-02,  4.4645e-01,  8.4723e-02,  ..., -2.1907e-01,\n           -1.3717e-01,  2.6413e-01],\n          [ 1.8776e-01,  6.9736e-03, -5.5637e-01,  ..., -2.8416e-02,\n            2.5515e-01,  4.6580e-01]],\n\n         [[ 8.1063e-03, -1.7806e-02, -3.1848e-01,  ...,  1.4365e-01,\n           -3.7737e-02,  1.2483e-01],\n          [-8.5387e-02,  1.6120e-01, -2.7067e-01,  ..., -2.4433e-01,\n           -9.3544e-02, -1.0003e-01],\n          [-3.2188e-01, -3.6161e-01, -3.3112e-01,  ..., -6.0482e-01,\n           -3.0834e-01,  2.4398e-02],\n          ...,\n          [ 1.2053e-01, -3.9799e-01,  1.0407e-01,  ...,  1.5394e+00,\n            4.5902e-01, -9.6906e-01],\n          [-7.4711e-01, -1.7102e-01, -8.2709e-01,  ..., -4.9746e-01,\n           -1.2640e-01,  4.3773e-02],\n          [-2.2919e-01, -1.2267e-01, -7.6370e-01,  ..., -3.8480e-01,\n            7.0970e-02,  2.4827e-01]]],\n\n\n        [[[-4.8104e-02, -3.9557e-01, -2.7095e-01,  ..., -3.4844e-01,\n           -2.3906e-01, -2.5550e-01],\n          [-1.9117e-02, -1.1523e-01, -1.6701e-01,  ...,  1.0418e-01,\n           -1.3509e-01, -1.6501e-01],\n          [-4.3906e-02, -7.0913e-02, -1.5929e-01,  ..., -7.6162e-02,\n            1.3345e-02, -4.6048e-02],\n          ...,\n          [ 3.1323e-01,  4.0539e-02, -1.3182e-01,  ...,  2.3887e-01,\n           -3.0051e-01, -3.9242e-02],\n          [ 2.2863e-03,  1.0602e-01, -1.9749e-01,  ...,  3.4189e-01,\n           -1.8437e-01, -4.1512e-02],\n          [ 1.4937e-01,  4.2384e-01,  1.4425e-01,  ...,  1.6882e-01,\n            1.9218e-01, -3.8202e-02]],\n\n         [[-9.2120e-02, -2.4386e-01, -2.3827e-01,  ...,  1.0953e-01,\n           -4.4733e-01, -6.8849e-01],\n          [-4.1938e-01, -5.1203e-01, -5.5262e-01,  ..., -3.6812e-01,\n           -7.4945e-01, -7.8304e-01],\n          [-5.3313e-01, -4.2840e-01, -6.3894e-01,  ..., -1.6120e-01,\n           -4.2165e-01, -6.8779e-01],\n          ...,\n          [-6.3385e-01, -3.9571e-01, -5.1120e-01,  ...,  7.5354e-01,\n           -8.9978e-01, -6.8837e-01],\n          [-3.6109e-01, -2.2821e-01, -1.8437e-01,  ..., -4.8366e-01,\n            5.1462e-02, -3.0329e-01],\n          [-4.1273e-01, -4.4439e-01, -4.4651e-01,  ..., -7.4278e-01,\n           -3.2459e-01, -3.7495e-01]],\n\n         [[ 1.1926e-01,  9.5121e-02,  1.5751e-01,  ..., -1.3779e-02,\n            7.0230e-02,  2.0632e-01],\n          [ 2.6625e-01,  5.1537e-02,  9.5987e-02,  ..., -4.4795e-01,\n            1.8810e-01,  2.2377e-01],\n          [ 5.1145e-01,  1.8852e-01,  2.0161e-01,  ..., -1.9842e-01,\n            1.2217e-02,  2.7290e-01],\n          ...,\n          [ 3.3658e-01,  3.6763e-01,  1.3941e-01,  ..., -4.4949e-02,\n            3.4857e-01, -4.3137e-02],\n          [ 4.0758e-01,  6.4796e-02,  2.4149e-01,  ...,  3.0350e-02,\n            3.4041e-01,  4.0712e-01],\n          [ 4.5716e-01,  3.0545e-01,  3.6638e-01,  ...,  1.5763e-01,\n            3.4303e-01,  1.5706e-01]],\n\n         ...,\n\n         [[-6.7573e-02,  8.3438e-02,  2.4324e-01,  ...,  1.4158e-02,\n           -2.5162e-01,  1.6434e-01],\n          [-2.4061e-02,  1.7137e-01,  1.0950e-01,  ...,  3.2367e-01,\n           -3.1223e-01,  1.3555e-01],\n          [-1.9832e-01,  5.0264e-02,  3.5868e-02,  ...,  1.3552e-01,\n            1.2958e-01,  2.0913e-01],\n          ...,\n          [-4.6597e-03,  4.9366e-02,  1.8714e-01,  ...,  1.0177e+00,\n           -1.6098e-01,  2.3058e-01],\n          [ 7.5481e-02,  1.4444e-01,  2.8364e-02,  ...,  1.5517e-01,\n           -1.2283e-01,  3.7836e-02],\n          [ 1.6428e-01,  3.0505e-01,  1.3647e-01,  ..., -2.8294e-01,\n            5.5076e-02, -5.7954e-02]],\n\n         [[-1.0970e-01,  1.8579e-01,  4.2916e-02,  ..., -1.3882e-01,\n            5.9157e-02,  1.9676e-01],\n          [-8.3217e-02, -1.1038e-01, -2.5567e-01,  ..., -6.0023e-01,\n           -3.7114e-01,  3.0515e-02],\n          [-1.2109e-01, -9.9213e-02, -2.0169e-01,  ..., -4.2881e-01,\n           -4.3398e-01, -1.2138e-01],\n          ...,\n          [-1.5314e-02,  2.7062e-01, -1.8329e-01,  ..., -3.9080e-01,\n           -2.5083e-01,  2.2430e-01],\n          [ 2.1567e-01, -3.0287e-02, -2.9929e-01,  ..., -8.5693e-01,\n           -2.4367e-01,  7.8193e-02],\n          [ 2.5349e-01,  1.1735e-02, -4.9794e-02,  ..., -7.9913e-01,\n           -1.6857e-01,  2.0306e-01]],\n\n         [[ 1.3910e-02, -1.4393e-01, -2.4672e-01,  ..., -2.9001e-01,\n           -1.5070e-01,  1.5505e-01],\n          [ 9.7321e-02,  9.0710e-02, -3.9046e-01,  ..., -3.6714e-01,\n           -1.0289e-01, -1.8730e-02],\n          [-3.6524e-01, -3.3544e-01, -4.1749e-01,  ...,  2.3299e-01,\n            1.9160e-01,  6.7667e-02],\n          ...,\n          [-3.7399e-02,  9.4682e-02, -4.0049e-01,  ..., -5.1874e-01,\n           -3.3716e-01, -1.5224e-02],\n          [-1.4139e-01, -2.3622e-01, -1.4169e-01,  ..., -1.7983e-02,\n           -1.0865e-02, -8.3069e-03],\n          [ 2.0323e-01,  3.0407e-01, -2.8859e-02,  ..., -1.5721e+00,\n            4.3957e-01, -1.3377e-01]]]], grad_fn=<AddBackward0>)])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs.values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "body5outputsvalues = list(body5outputs.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 40, 105, 105])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 80, 53, 53])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 160, 27, 27])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from models.retinaface import RetinaFace\n",
    "\n",
    "cfg = cfg_mnet\n",
    "\n",
    "mnet1 = RetinaFace(cfg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    mnet1outputs = mnet1(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(mnet1outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 4])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 2])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 10])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from models.net import MobileNetV1 as MobileNetV1\n",
    "from models.net import FPN as FPN\n",
    "from models.net import SSH as SSH\n",
    "\n",
    "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_bn_no_relu(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride, leaky=0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "    )\n",
    "\n",
    "class SSH_mv3(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(SSH, self).__init__()\n",
    "        assert out_channel % 4 == 0\n",
    "        leaky = 0\n",
    "        if (out_channel <= 64):\n",
    "            leaky = 0.1\n",
    "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
    "\n",
    "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv3X3 = self.conv3X3(input)\n",
    "\n",
    "        conv5X5_1 = self.conv5X5_1(input)\n",
    "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
    "\n",
    "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
    "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
    "\n",
    "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)\n",
    "\n",
    "\n",
    "class RetinaFace_mobilenetv3lg(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace_mobilenetv3lg,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        backbone = torchvision.models.mobilenet_v3_large()\n",
    "        # backbone.features[4].block[3][0].stride=(1,1)\n",
    "        # backbone.features[9].block[3][0].stride=(1,1)\n",
    "        print(backbone)\n",
    "\n",
    "        self.body = _utils.IntermediateLayerGetter(backbone.features, {'6':0, '10':1, '15':2})\n",
    "        # in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            40,\n",
    "            80,\n",
    "            160,\n",
    "        ]\n",
    "        # out_channels = cfg['out_channel']\n",
    "        out_channels = 64\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=out_channels)\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=out_channels)\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=out_channels)\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "\n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        features = [feature1, feature2, feature3]\n",
    "        print(\"feature1.shape: \", feature1.shape)\n",
    "        print(\"feature2.shape: \", feature2.shape)\n",
    "        print(\"feature3.shape: \", feature3.shape)\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): ConvBNActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnetv3 = RetinaFace_mobilenetv3lg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1.shape:  torch.Size([2, 64, 105, 105])\n",
      "feature2.shape:  torch.Size([2, 64, 53, 53])\n",
      "feature3.shape:  torch.Size([2, 64, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mnetv3outputs = mnetv3(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnetv3outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 4])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 2])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 10])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "cfgres = cfg_re50\n",
    "\n",
    "res50fnet = RetinaFace(cfgres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1.shape:  torch.Size([2, 256, 105, 105])\n",
      "feature2.shape:  torch.Size([2, 256, 53, 53])\n",
      "feature3.shape:  torch.Size([2, 256, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    res50fnetoutputs = res50fnet(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res50fnetoutputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 4])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 2])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 10])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([9, 15])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 15])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-493cc119",
   "language": "python",
   "display_name": "PyCharm (programmers assignment ai 1)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}