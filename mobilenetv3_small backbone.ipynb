{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "import torchvision.models._utils as _utils\n",
    "\n",
    "mov3sm = torchvision.models.mobilenet_v3_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): ConvBNActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mov3sm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# 64 128 256\n",
    "# 64 128 256 512\n",
    "# 16 24 40 48 96\n",
    "\n",
    "# 40 48 96\n",
    "body5 = _utils.IntermediateLayerGetter(mov3sm.features, {'4':0, '7':1, '11':2})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "IntermediateLayerGetter(\n  (0): ConvBNActivation(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n    (2): Hardswish()\n  )\n  (1): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): SqueezeExcitation(\n        (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (2): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (3): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): ConvBNActivation(\n        (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (4): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (5): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (6): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (7): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (8): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (9): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (10): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n  (11): InvertedResidual(\n    (block): Sequential(\n      (0): ConvBNActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): ConvBNActivation(\n        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (2): SqueezeExcitation(\n        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n        (relu): ReLU(inplace=True)\n        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (3): ConvBNActivation(\n        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Identity()\n      )\n    )\n  )\n)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from data import WiderFaceDetection, detection_collate, preproc, cfg_mnet, cfg_re50\n",
    "\n",
    "img_dim = 840\n",
    "rgb_mean = (104, 117, 123) # bgr order\n",
    "\n",
    "dataset = WiderFaceDetection( './data/widerface/train/label.txt',preproc(img_dim, rgb_mean))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "batch_iterator = iter(data.DataLoader(dataset, 2, shuffle=True, num_workers=4, collate_fn=detection_collate))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "images, targets = next(batch_iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 840, 840])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[0.6300, 0.0920, 0.6540, 0.1420, 0.6380, 0.1080, 0.6400, 0.1100, 0.6300,\n          0.1140, 0.6320, 0.1260, 0.6340, 0.1260, 1.0000],\n         [0.5120, 0.0360, 0.5660, 0.1280, 0.5329, 0.0688, 0.5353, 0.0706, 0.5115,\n          0.0851, 0.5173, 0.1031, 0.5225, 0.1042, 1.0000],\n         [0.1480, 0.1320, 0.1700, 0.1720, 0.1560, 0.1440, 0.1580, 0.1440, 0.1460,\n          0.1500, 0.1540, 0.1580, 0.1520, 0.1580, 1.0000],\n         [0.0020, 0.0460, 0.0520, 0.1060, 0.0214, 0.0642, 0.0302, 0.0566, 0.0364,\n          0.0689, 0.0352, 0.0857, 0.0414, 0.0827, 1.0000]]),\n tensor([[0.7122, 0.0203, 0.8983, 0.2645, 0.7449, 0.1163, 0.8283, 0.1225, 0.7681,\n          0.1657, 0.7495, 0.1950, 0.8206, 0.2012, 1.0000],\n         [0.4215, 0.0378, 0.6134, 0.1948, 0.5008, 0.0641, 0.5732, 0.1052, 0.5153,\n          0.0979, 0.4718, 0.1257, 0.5092, 0.1474, 1.0000],\n         [0.0262, 0.0203, 0.1831, 0.2471, 0.0571, 0.1193, 0.1306, 0.1193, 0.0888,\n          0.1726, 0.0557, 0.1856, 0.1378, 0.1884, 1.0000]])]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([(0,\n              tensor([[[[-1.0287, -0.2955, -0.8011,  ..., -0.6917, -0.3188, -0.1718],\n                        [-0.8955, -0.4953, -0.2492,  ..., -1.3434, -0.6797, -0.5948],\n                        [ 0.1749,  0.8077,  0.0800,  ..., -0.4142, -1.0549, -0.2239],\n                        ...,\n                        [-0.0838, -0.6984,  0.2127,  ...,  0.8075,  0.8470, -0.3681],\n                        [ 0.0345, -0.4816,  0.2770,  ...,  0.3087,  0.5350,  0.1047],\n                        [ 0.0804, -0.0887,  0.6788,  ...,  0.2320,  0.6985, -0.1012]],\n              \n                       [[-0.8315,  0.6266, -0.2660,  ..., -0.7062,  0.7801,  0.5138],\n                        [ 0.4540, -1.8000, -0.7581,  ..., -1.2792,  0.1847, -1.1135],\n                        [-0.1133, -1.6506, -0.6727,  ...,  1.6151,  0.3576,  0.6341],\n                        ...,\n                        [-0.3389,  0.7374,  0.2495,  ...,  0.4880,  0.3129,  0.1078],\n                        [-0.3701,  1.2801, -0.0799,  ...,  0.4561,  0.3373,  0.3979],\n                        [-0.4876,  0.8539,  0.1361,  ...,  0.3005,  0.3353,  0.2256]],\n              \n                       [[-0.2215,  0.2319,  0.1879,  ...,  1.5161, -0.6371,  0.1451],\n                        [-0.2981, -1.3545,  1.5033,  ...,  0.7556, -0.0550,  0.6088],\n                        [-0.6511, -0.2649,  0.6437,  ...,  0.2999,  0.6550,  0.1922],\n                        ...,\n                        [-0.1564, -0.7827,  0.6923,  ...,  0.2748,  1.0509,  0.7681],\n                        [-0.5478, -0.0851,  0.5796,  ...,  0.4745,  0.4996,  0.6518],\n                        [ 0.3699,  0.0244,  0.6017,  ...,  0.3085,  0.5105,  0.4283]],\n              \n                       ...,\n              \n                       [[ 0.0634,  0.5854,  0.9941,  ...,  0.8964,  1.3299,  1.5531],\n                        [-0.6750, -0.6018,  0.3513,  ..., -0.1988, -0.4232,  0.1111],\n                        [-0.0077, -0.1272,  1.1197,  ...,  0.4349, -0.6377,  0.1911],\n                        ...,\n                        [ 0.3183, -0.7791,  0.3330,  ...,  0.8526, -0.4304,  0.8313],\n                        [ 0.2952, -0.9942,  0.2943,  ...,  0.1016,  0.1971,  0.7693],\n                        [ 0.6752, -0.7148,  0.2436,  ..., -0.0936, -0.0708,  0.1546]],\n              \n                       [[ 1.1424, -1.0337,  0.0694,  ..., -0.5133,  0.7732,  0.1662],\n                        [-0.0822,  0.3061, -0.6355,  ..., -0.5224,  0.2382,  0.4777],\n                        [ 0.2244, -0.2425, -0.5626,  ...,  0.5499,  0.3178,  1.0116],\n                        ...,\n                        [-0.3516, -0.3008,  0.3606,  ..., -0.0081,  0.5906,  0.2738],\n                        [ 0.3387, -0.3804,  0.3029,  ...,  0.1545,  0.2883,  0.4627],\n                        [ 0.1185,  0.4604,  0.6399,  ...,  0.5155,  0.7655,  0.3558]],\n              \n                       [[-0.4309, -0.4202, -0.3691,  ..., -0.7153,  0.5310,  0.6808],\n                        [ 0.7168, -1.2828, -0.1594,  ..., -1.1307,  1.2933,  0.0945],\n                        [ 0.3065, -1.9876,  1.0138,  ...,  0.1900,  0.5670,  0.4609],\n                        ...,\n                        [ 0.1393, -0.0216,  0.6440,  ...,  0.6762,  0.5203,  0.4187],\n                        [ 0.5787,  0.5075,  0.5131,  ...,  0.5465,  0.5530,  0.5576],\n                        [ 0.7819,  0.8778,  1.1265,  ...,  1.0488,  1.0520,  0.3789]]],\n              \n              \n                      [[[-1.1460, -0.0086, -0.9435,  ..., -0.4868, -0.8850, -2.3676],\n                        [-2.4209,  1.3577, -2.2423,  ..., -0.5165,  0.4711,  0.2811],\n                        [-1.8280, -0.4527, -1.1512,  ..., -1.0491,  0.1157,  0.0205],\n                        ...,\n                        [ 4.3264,  3.2576,  0.7792,  ...,  0.9579,  0.6284, -0.7256],\n                        [ 1.5644, -0.0838, -0.2864,  ..., -0.6858,  0.0192, -0.2577],\n                        [ 0.8961,  1.0824,  0.0340,  ...,  0.6214,  0.2506, -1.0681]],\n              \n                       [[ 1.7426,  0.8702, -1.1420,  ...,  2.7234,  1.7371, -0.1690],\n                        [-1.9686, -3.6233, -5.3411,  ..., -2.6105, -3.8278, -4.7645],\n                        [-0.2818, -1.9184,  1.2466,  ..., -0.3337, -0.4677, -0.7827],\n                        ...,\n                        [-4.3306, -3.6189,  1.3716,  ..., -1.3631, -1.4451, -0.6732],\n                        [-2.9002, -2.5899, -0.2991,  ..., -1.6918, -0.8160, -1.4317],\n                        [-5.4331, -2.5449, -0.6531,  ..., -0.1730, -0.1040, -0.6754]],\n              \n                       [[ 0.1000, -0.7292, -0.1229,  ..., -1.7251, -1.2660,  0.2984],\n                        [-5.4692, -2.9505, -1.8133,  ..., -1.6670, -2.0202, -0.3307],\n                        [-2.3113,  0.3050,  0.1196,  ..., -0.6474, -0.1054, -0.1248],\n                        ...,\n                        [-2.5566, -3.6462, -1.1961,  ..., -0.7776, -0.9033,  0.3349],\n                        [-1.0529, -3.1207,  0.1760,  ..., -0.5122,  1.7838,  0.2734],\n                        [-2.8754, -3.9855,  0.6290,  ..., -1.0319,  0.6856,  0.1964]],\n              \n                       ...,\n              \n                       [[ 0.3053, -1.1211, -0.0201,  ..., -0.0746,  1.1226, -0.1402],\n                        [-1.1416, -2.2932, -0.9449,  ..., -1.2163, -1.1067, -3.4312],\n                        [-1.3087, -0.0973, -1.0917,  ...,  0.2440,  0.8091,  0.0142],\n                        ...,\n                        [-4.3562, -2.9125, -1.8075,  ..., -1.2341,  0.1078, -0.4077],\n                        [-2.9856, -1.8455,  0.0426,  ..., -0.9032, -0.0827,  0.2624],\n                        [-2.6225, -0.1891, -0.4059,  ..., -2.6634,  1.6707, -0.5098]],\n              \n                       [[ 1.2235,  0.6893, -1.5773,  ..., -2.0042, -1.2727, -2.0661],\n                        [ 2.2190,  1.1529,  0.6448,  ..., -1.8717, -2.2940,  0.9264],\n                        [ 2.9375,  1.0229, -0.4757,  ...,  0.2873, -0.2784, -0.7856],\n                        ...,\n                        [ 1.2231, -2.3697, -0.4436,  ..., -1.1787, -0.8798, -1.3351],\n                        [ 2.7396, -0.4413, -0.7323,  ...,  0.5778, -2.0671,  0.1106],\n                        [ 2.4041,  0.7290,  0.2351,  ..., -0.0568,  0.1510,  0.6330]],\n              \n                       [[-1.2342, -1.0011, -1.3012,  ..., -2.6316, -2.1930, -1.6530],\n                        [-1.8267, -0.3688, -0.6976,  ...,  1.3773,  0.1266, -0.5905],\n                        [-1.9685, -0.4222, -0.1939,  ...,  0.3526, -0.3322, -0.9651],\n                        ...,\n                        [ 2.5133, -4.5988, -1.0577,  ..., -0.7032,  0.1345,  0.0588],\n                        [ 1.9380, -1.6808, -0.0124,  ..., -2.9726, -1.8994,  0.4708],\n                        [ 1.1411, -0.6895, -0.5149,  ..., -1.4069,  0.4963,  0.3225]]]],\n                     grad_fn=<NativeBatchNormBackward>)),\n             (1,\n              tensor([[[[ 1.1172e+00,  7.4981e-01,  1.2046e+00,  ..., -3.1743e-02,\n                          2.5929e-01,  5.1025e-03],\n                        [ 1.4760e+00,  6.8694e-01,  2.6656e-01,  ...,  6.8918e-01,\n                          4.5132e-05, -1.0145e-01],\n                        [ 1.4698e+00,  8.6762e-01,  1.2058e+00,  ..., -6.6776e-01,\n                          7.5373e-01, -9.8468e-02],\n                        ...,\n                        [ 4.5003e-01, -6.0279e-01, -8.0085e-01,  ..., -7.8238e-01,\n                         -4.5939e-01, -7.4045e-02],\n                        [ 5.7254e-01, -1.2816e-01, -1.6136e-01,  ..., -2.1314e-01,\n                         -1.7074e-01, -1.7125e-02],\n                        [ 4.5440e-01, -5.2869e-03, -3.9173e-01,  ..., -8.5364e-02,\n                         -8.2608e-02,  5.8193e-02]],\n              \n                       [[ 2.6749e-01,  1.8167e-01, -4.2307e-01,  ..., -2.1237e-01,\n                         -6.2031e-01, -7.0485e-01],\n                        [ 6.4827e-01,  1.3112e-01,  7.8458e-01,  ...,  1.5867e-01,\n                          8.3294e-02, -8.5470e-01],\n                        [ 5.3363e-01, -7.9341e-01,  1.2851e-02,  ..., -9.4777e-01,\n                          4.4539e-01,  5.5548e-01],\n                        ...,\n                        [ 6.6349e-01, -3.9594e-01, -8.0307e-01,  ..., -6.9387e-01,\n                         -8.0722e-01, -4.8614e-01],\n                        [ 3.8363e-01, -3.5606e-01, -2.6217e-01,  ..., -6.1895e-01,\n                         -6.0896e-01, -5.3620e-01],\n                        [ 4.4808e-03, -2.6094e-01, -1.8678e-01,  ..., -3.6214e-01,\n                         -4.0434e-01, -2.4164e-01]],\n              \n                       [[ 1.7491e-01,  5.3317e-01,  5.5895e-01,  ..., -2.9013e-01,\n                         -3.3383e-01, -4.0377e-01],\n                        [-1.1576e-01,  1.8699e-01,  7.6024e-01,  ..., -4.1623e-01,\n                         -3.5394e-01,  1.4107e-01],\n                        [ 1.8757e-01,  9.1835e-01,  1.7958e+00,  ..., -1.1508e+00,\n                          8.5222e-02, -7.9480e-01],\n                        ...,\n                        [-2.4218e-01, -5.5397e-01, -4.1694e-01,  ..., -6.6256e-01,\n                         -9.3807e-01, -7.3335e-01],\n                        [-1.8712e-01, -7.6005e-01, -5.8281e-01,  ..., -6.4202e-01,\n                         -8.4059e-01, -5.4237e-01],\n                        [-3.7486e-01, -7.8660e-01, -7.0951e-01,  ..., -6.9650e-01,\n                         -6.7512e-01, -1.9945e-01]],\n              \n                       ...,\n              \n                       [[-7.2302e-01, -6.4162e-01, -6.2527e-01,  ...,  1.6937e-01,\n                          3.6349e-02, -5.9554e-01],\n                        [ 1.2312e-01, -7.3516e-01, -2.1544e-02,  ..., -2.2992e-01,\n                         -1.1033e-01, -7.9800e-01],\n                        [ 6.6940e-01,  1.1531e+00,  5.9935e-01,  ..., -1.2643e+00,\n                         -8.9780e-01,  7.4302e-01],\n                        ...,\n                        [ 2.1477e-01, -8.3981e-02,  9.1120e-02,  ...,  7.4198e-02,\n                         -8.0394e-03,  3.2258e-01],\n                        [-2.2963e-01, -4.6281e-01, -9.3862e-02,  ..., -1.6741e-01,\n                         -4.9481e-01,  4.2790e-02],\n                        [ 2.2239e-01, -2.2373e-02,  1.8849e-01,  ..., -1.5082e-01,\n                         -7.8535e-02,  4.9437e-02]],\n              \n                       [[ 2.5514e-01,  4.1261e-02,  4.2561e-01,  ...,  2.8126e-02,\n                         -3.4343e-01,  9.6607e-02],\n                        [ 1.5462e+00,  8.0926e-01,  7.5365e-01,  ..., -2.3063e-01,\n                          1.5992e-01, -6.3655e-01],\n                        [ 7.7001e-01,  6.0739e-01,  9.5067e-01,  ...,  1.9835e-01,\n                          8.9161e-01, -7.9258e-01],\n                        ...,\n                        [ 9.7178e-02, -5.1052e-01, -4.6044e-01,  ..., -3.6277e-01,\n                         -2.3927e-01,  1.8679e-01],\n                        [ 1.8511e-01, -4.1707e-01, -5.7256e-01,  ..., -4.8846e-02,\n                          1.4768e-02,  2.9032e-01],\n                        [-2.5991e-01, -2.0361e-01, -7.4765e-02,  ..., -5.2902e-02,\n                         -8.5396e-02,  2.3602e-01]],\n              \n                       [[ 1.8429e-01, -3.5593e-01, -4.9465e-01,  ...,  6.6760e-01,\n                         -5.9303e-01,  1.7976e-01],\n                        [-4.0298e-01, -5.7816e-01,  1.0592e+00,  ...,  4.5332e-01,\n                         -8.3964e-02, -2.6185e-01],\n                        [-2.3055e-01,  1.2753e+00,  1.6678e+00,  ...,  3.8045e-01,\n                         -1.2317e-01,  7.4330e-01],\n                        ...,\n                        [ 2.8183e-01,  2.2476e-01,  4.6220e-01,  ...,  1.1424e-01,\n                          1.4642e-01, -9.9893e-02],\n                        [ 1.9206e-01,  3.6839e-01,  1.4861e-03,  ..., -9.1912e-02,\n                          2.6664e-01, -5.4116e-02],\n                        [ 2.0170e-01,  2.5296e-01,  2.9536e-02,  ...,  1.0873e-01,\n                          2.5807e-01,  1.3572e-01]]],\n              \n              \n                      [[[ 1.9900e+00,  5.1765e-02,  1.7125e+00,  ...,  1.2835e+00,\n                         -1.6105e-01, -6.6695e-01],\n                        [ 6.2994e-01,  2.0898e+00,  1.6947e+00,  ..., -7.3369e-01,\n                          6.6773e-01,  4.5965e-01],\n                        [ 1.2657e-01, -1.0415e+00,  3.1800e+00,  ...,  1.0279e+00,\n                          1.0241e-02,  2.7184e-01],\n                        ...,\n                        [ 1.6799e-01, -4.0279e+00, -1.9414e+00,  ...,  4.7549e-01,\n                         -5.4896e-01,  1.0898e+00],\n                        [-7.5195e-01, -7.3816e-01, -1.9451e+00,  ...,  4.3247e-01,\n                          6.3246e-01,  2.6599e-01],\n                        [-8.0009e-01,  1.2214e+00, -1.5957e+00,  ...,  3.6704e-01,\n                          2.6712e-01,  4.6265e-01]],\n              \n                       [[ 1.7575e+00,  3.9604e-02,  5.7028e-01,  ...,  5.0569e-01,\n                         -9.1600e-01, -4.7875e-01],\n                        [ 4.1478e-01, -5.2835e-01, -7.3475e-01,  ...,  8.8511e-01,\n                         -1.6976e+00, -7.8147e-01],\n                        [-1.6333e+00,  6.2205e-01, -2.3161e-01,  ...,  8.8553e-01,\n                         -2.0004e-01,  3.8043e-01],\n                        ...,\n                        [-8.7323e-01, -1.8280e+00, -2.9440e+00,  ...,  1.3942e-01,\n                         -2.3488e-01, -4.0187e-01],\n                        [-1.6943e+00, -3.2042e+00,  2.8109e-01,  ..., -1.0525e-02,\n                          2.4725e-01,  4.1285e-01],\n                        [-3.5837e+00,  1.0943e+00, -1.7693e-01,  ...,  4.9952e-01,\n                          3.2284e-01,  3.3321e-01]],\n              \n                       [[ 1.3631e+00,  8.7395e-01,  1.9501e+00,  ...,  1.6150e+00,\n                          5.6555e-01, -1.3966e-01],\n                        [ 5.5708e-01,  2.8404e+00,  2.1415e+00,  ..., -9.9308e-01,\n                         -1.6287e+00, -1.0610e+00],\n                        [ 6.5834e-01,  1.0221e+00,  3.8784e+00,  ...,  3.6102e-01,\n                          3.1111e-01, -3.2842e-02],\n                        ...,\n                        [-9.5814e-01,  2.3862e-01, -9.3930e-01,  ..., -3.4848e-01,\n                          9.0627e-01,  1.9039e-01],\n                        [ 1.2494e+00,  8.2028e-01,  9.0438e-01,  ...,  2.4699e-01,\n                         -8.8405e-01, -5.9188e-01],\n                        [ 1.6400e+00,  4.3728e-01, -6.6048e-01,  ...,  1.4251e-01,\n                          1.0364e+00, -1.0920e-01]],\n              \n                       ...,\n              \n                       [[ 1.4660e+00,  1.5815e-01,  2.5063e-01,  ...,  1.6219e+00,\n                          2.1034e+00,  1.0430e+00],\n                        [ 6.7504e-01, -1.5781e+00,  1.9398e+00,  ..., -1.2717e+00,\n                         -3.0854e-01, -7.1286e-01],\n                        [ 2.3650e+00,  2.8090e+00,  1.8964e+00,  ...,  4.6642e-01,\n                          1.6101e+00, -1.8562e+00],\n                        ...,\n                        [ 5.9219e-01, -1.6804e+00, -3.5421e+00,  ...,  3.8854e-01,\n                          6.0735e-01, -3.3260e-01],\n                        [ 1.2847e-01,  3.8837e-01, -1.2657e+00,  ...,  1.0884e+00,\n                          1.1005e+00,  8.0092e-01],\n                        [-1.5151e+00,  1.4858e+00,  5.4771e-01,  ..., -1.5389e+00,\n                          3.6289e-01,  1.5500e-01]],\n              \n                       [[ 4.2762e-02,  2.4055e+00,  7.1834e-01,  ..., -5.4720e-01,\n                         -3.5952e-01,  3.3186e-01],\n                        [-1.2750e+00, -3.8623e-01,  1.2904e+00,  ..., -1.1431e+00,\n                         -4.8703e-01, -1.1221e+00],\n                        [ 2.7821e+00, -2.6252e-01,  1.5799e+00,  ...,  6.8726e-02,\n                         -6.4505e-01, -2.3601e-02],\n                        ...,\n                        [-1.8065e+00,  5.4329e-01,  4.0761e+00,  ..., -8.8423e-01,\n                         -6.9876e-01, -5.3562e-01],\n                        [-7.0744e-01,  7.6289e-01, -7.9512e-01,  ..., -2.0068e+00,\n                         -1.3336e+00, -1.1010e+00],\n                        [-7.2813e-01, -1.5471e+00, -6.7800e-01,  ..., -1.1903e+00,\n                         -8.1946e-01, -7.7718e-01]],\n              \n                       [[-9.9440e-02, -5.6966e-01,  1.8694e-01,  ..., -7.5649e-01,\n                         -1.2679e+00,  3.0182e-01],\n                        [-1.6360e+00,  2.1977e-02,  5.4589e-01,  ..., -2.2600e+00,\n                         -5.8394e-01,  7.8615e-01],\n                        [-2.3373e+00,  1.7132e+00, -2.3140e+00,  ...,  3.2710e-01,\n                          4.6386e-01,  1.4422e+00],\n                        ...,\n                        [-2.2469e+00, -2.8675e+00, -1.3352e+00,  ..., -2.4725e-01,\n                          1.4316e-01, -2.9669e-01],\n                        [-4.5405e+00,  1.5960e-01, -7.3387e-01,  ...,  3.6575e-01,\n                         -2.3077e-01,  1.3253e+00],\n                        [-4.5734e+00, -2.3272e-01, -9.8195e-01,  ...,  6.6379e-01,\n                         -1.0771e+00,  1.0032e+00]]]], grad_fn=<NativeBatchNormBackward>)),\n             (2,\n              tensor([[[[ 2.6818e-01, -3.0713e-01,  9.3296e-01,  ..., -7.2070e-02,\n                          5.6623e-01,  9.4032e-01],\n                        [ 9.9202e-01, -3.1718e-01,  6.6551e-02,  ...,  3.5329e-01,\n                          5.7826e-01,  1.2057e+00],\n                        [ 8.3137e-02,  1.1910e+00,  3.4264e+00,  ...,  1.4008e-01,\n                          5.2098e-01,  9.6077e-01],\n                        ...,\n                        [-2.6047e-02, -9.8307e-02,  3.4512e-02,  ..., -5.5310e-01,\n                         -8.8283e-01,  3.9849e-01],\n                        [ 4.1768e-01,  2.0427e-01, -8.1389e-03,  ..., -6.0479e-01,\n                         -2.5219e-01, -4.5997e-01],\n                        [ 7.4815e-01,  7.6613e-01,  8.2971e-01,  ...,  8.9338e-01,\n                          4.1823e-01,  4.3994e-01]],\n              \n                       [[ 4.0596e-01,  1.2698e+00,  7.6740e-01,  ...,  2.6769e-01,\n                         -1.8367e-01, -4.2464e-02],\n                        [ 1.3783e+00,  9.6890e-01,  1.9408e+00,  ..., -9.9554e-01,\n                          1.9038e-01, -2.1866e-01],\n                        [-2.3551e-01,  9.7896e-01,  1.7036e+00,  ..., -2.9334e-01,\n                          2.6411e-01, -8.7048e-04],\n                        ...,\n                        [-5.8164e-02,  1.9689e-01,  3.4615e-02,  ...,  5.4427e-01,\n                          5.4060e-01,  2.2820e-01],\n                        [ 1.0815e-01, -1.0593e-01,  4.1619e-01,  ...,  1.5443e-01,\n                          1.8637e-01,  5.2127e-01],\n                        [ 2.6987e-01,  2.9454e-01,  7.8332e-01,  ...,  6.3216e-01,\n                          7.5313e-01,  7.2208e-01]],\n              \n                       [[-3.3481e-01,  3.2153e-01, -3.0891e-01,  ..., -7.1998e-01,\n                          8.4450e-01,  8.6633e-01],\n                        [-2.3619e-01,  1.1991e+00,  4.2846e-01,  ...,  5.3036e-01,\n                          1.0558e+00,  1.6286e-01],\n                        [ 2.8705e-01, -1.1575e+00,  2.1492e+00,  ...,  8.5733e-01,\n                         -3.0811e-01,  1.5864e-01],\n                        ...,\n                        [ 9.6970e-02,  5.6615e-01,  6.2407e-01,  ...,  9.7744e-01,\n                          6.3684e-01, -1.2539e-01],\n                        [ 3.8559e-01,  5.3742e-01,  5.1831e-01,  ...,  8.1235e-01,\n                          5.0940e-01, -3.5398e-02],\n                        [ 7.2196e-01,  4.4926e-01,  7.2383e-02,  ...,  3.1538e-01,\n                         -1.1604e-02,  4.9518e-02]],\n              \n                       ...,\n              \n                       [[-8.2825e-01,  8.5150e-02, -6.4795e-01,  ..., -1.5482e-01,\n                         -1.0965e+00, -1.9694e-01],\n                        [ 1.8386e-01,  8.7270e-01, -4.9010e-02,  ..., -1.1872e-01,\n                         -6.6697e-01,  2.9162e-01],\n                        [ 2.0702e+00,  4.6539e+00,  6.9686e-01,  ..., -6.2576e-02,\n                          7.1149e-01,  1.4492e-01],\n                        ...,\n                        [-6.7642e-01, -8.2948e-01, -6.3322e-01,  ..., -4.1821e-01,\n                          1.0440e-01,  9.7716e-01],\n                        [-7.5116e-01, -7.7098e-01, -5.2579e-01,  ...,  7.0998e-01,\n                          9.1478e-01,  6.2572e-01],\n                        [-2.2435e-01, -1.1725e-01,  2.1530e-01,  ...,  1.2347e-01,\n                          5.6903e-01,  2.5990e-01]],\n              \n                       [[ 4.6337e-01,  9.3301e-01,  1.3170e+00,  ...,  1.2774e-01,\n                          1.4027e-01,  7.1617e-01],\n                        [ 1.0716e+00, -6.2195e-01,  8.1885e-01,  ...,  7.8072e-01,\n                          6.0195e-02,  1.0340e+00],\n                        [ 2.1478e-01, -1.4699e+00, -3.0222e-01,  ...,  7.7277e-01,\n                          5.5248e-01, -1.4620e-01],\n                        ...,\n                        [ 6.2491e-01,  9.2545e-01,  1.0776e+00,  ...,  6.2632e-01,\n                          3.3433e-01,  1.2395e-01],\n                        [ 2.4802e-01,  6.1924e-01,  7.3762e-01,  ...,  1.3609e-01,\n                          3.0907e-01,  3.7686e-01],\n                        [ 2.8122e-01,  4.1431e-01,  3.7476e-01,  ...,  3.5635e-01,\n                          3.1058e-01,  3.8955e-01]],\n              \n                       [[-1.4604e-01, -3.5260e-01,  7.9725e-01,  ...,  8.2296e-01,\n                          1.8789e-01,  3.6095e-01],\n                        [ 1.6302e+00, -5.5932e-01,  1.9608e+00,  ...,  9.5154e-01,\n                          7.8006e-01,  1.5199e+00],\n                        [-1.9360e-01,  1.7770e+00, -5.2922e-02,  ...,  9.4788e-01,\n                          1.0638e+00,  1.4937e+00],\n                        ...,\n                        [ 2.6404e-01,  4.2596e-01,  9.2848e-01,  ...,  1.8410e-01,\n                          1.2291e+00,  9.9495e-01],\n                        [ 6.5499e-01,  3.1627e-01,  2.8339e-01,  ...,  6.3482e-01,\n                          6.7647e-01,  8.8502e-01],\n                        [ 7.4596e-01,  7.1250e-01,  8.1583e-01,  ...,  1.0910e+00,\n                          1.0307e+00,  8.0060e-01]]],\n              \n              \n                      [[[ 3.9678e-02,  4.1267e-01,  1.2824e+00,  ...,  8.9393e-01,\n                          7.2904e-01, -8.0427e-02],\n                        [ 1.2885e+00, -5.8858e-01, -1.0585e+00,  ..., -4.8453e-01,\n                         -3.3399e-01,  1.3410e+00],\n                        [ 5.7542e-01, -3.0988e-02,  2.0391e+00,  ..., -1.6995e+00,\n                         -1.0538e+00, -1.2840e+00],\n                        ...,\n                        [-4.7013e-01,  2.6503e+00, -4.7412e+00,  ..., -3.1497e-01,\n                          6.2376e-01, -6.0068e-01],\n                        [ 1.0676e+00, -1.0027e+00,  1.2860e+00,  ...,  6.2559e-01,\n                         -2.5793e-01,  5.5135e-01],\n                        [ 6.7295e-01,  1.1658e+00, -2.2603e+00,  ...,  1.0291e+00,\n                          1.5602e+00,  6.5057e-01]],\n              \n                       [[-1.8994e-01,  1.0590e+00,  4.7198e-01,  ...,  6.6125e-01,\n                         -2.1813e+00,  2.6573e-01],\n                        [ 9.5922e-01,  3.3537e-01,  9.5515e-01,  ..., -1.0504e+00,\n                         -1.0392e+00,  7.0789e-01],\n                        [-2.1512e-01, -1.8410e+00, -1.1447e+00,  ..., -1.6028e+00,\n                         -2.0107e-01,  4.9469e-01],\n                        ...,\n                        [-9.0959e-01, -4.2687e-01, -1.7341e+00,  ...,  6.2345e-02,\n                          1.3490e+00, -5.7777e-01],\n                        [-1.2466e+00,  2.1889e+00, -1.2522e+00,  ...,  7.6104e-02,\n                          4.4413e-01,  4.3920e-02],\n                        [-2.8657e-01,  1.6151e+00,  2.5330e+00,  ..., -3.9641e-01,\n                          1.0055e+00, -2.2348e-01]],\n              \n                       [[-3.3373e-02,  9.3591e-01, -1.6262e-01,  ..., -1.8855e-01,\n                         -8.3778e-01, -1.1317e-01],\n                        [ 8.0296e-01, -2.1935e+00, -1.0744e+00,  ...,  2.4919e+00,\n                         -7.5187e-01, -6.1645e-01],\n                        [-4.0333e+00,  3.3154e+00,  3.4767e+00,  ..., -9.5695e-01,\n                          1.1324e+00, -7.8642e-01],\n                        ...,\n                        [ 4.9674e-01, -1.0956e+00,  3.0219e-01,  ...,  2.2823e-01,\n                          2.8386e-01, -5.7936e-01],\n                        [ 1.9749e+00,  2.8471e+00,  1.0825e-01,  ...,  1.9658e+00,\n                          2.5847e-01,  3.2147e-01],\n                        [-8.6962e-01,  8.8421e-01, -1.9340e+00,  ...,  7.0978e-01,\n                          1.3071e+00,  9.5451e-01]],\n              \n                       ...,\n              \n                       [[ 7.4017e-01, -3.5305e-01, -4.8652e-01,  ..., -8.2315e-01,\n                         -8.5266e-01, -1.2359e-01],\n                        [-4.3077e-02,  1.9267e+00, -8.8176e-01,  ...,  3.1720e-01,\n                          9.4731e-01, -1.4925e+00],\n                        [ 1.3360e+00, -3.2231e+00, -1.5778e+00,  ...,  1.4385e+00,\n                          8.9146e-01, -8.7649e-01],\n                        ...,\n                        [-1.0526e+00, -8.5502e-01, -1.0995e-01,  ...,  6.8825e-02,\n                         -4.6232e-01,  3.6191e-01],\n                        [ 5.0905e-01,  1.6061e+00,  2.9704e+00,  ..., -1.4136e-01,\n                         -6.4005e-01, -1.9797e-01],\n                        [ 5.4744e-01,  9.7695e-01,  1.1172e+00,  ...,  5.7680e-01,\n                         -6.8556e-02, -1.1736e+00]],\n              \n                       [[-7.0787e-01, -1.0261e-01, -1.4139e+00,  ..., -1.0090e+00,\n                          3.6000e-01, -4.4108e-01],\n                        [-1.0868e+00,  1.0141e-02,  3.6472e-01,  ...,  9.6627e-02,\n                          1.9707e+00,  4.1770e-01],\n                        [ 8.9345e-01, -3.4930e+00,  7.0642e-01,  ...,  2.2592e+00,\n                         -2.3000e+00, -3.7969e-01],\n                        ...,\n                        [ 1.7990e+00, -9.2122e-01,  3.8469e+00,  ...,  1.1233e+00,\n                          1.2448e+00,  2.4486e-01],\n                        [-2.8070e-01, -3.9467e+00,  7.3385e-01,  ...,  7.2980e-01,\n                          5.1622e-01, -6.6349e-02],\n                        [-6.3312e-01, -3.5453e-01, -8.5869e-01,  ...,  8.4925e-01,\n                         -3.5170e-02, -3.6832e-01]],\n              \n                       [[ 1.0652e+00,  7.0505e-01,  9.1074e-01,  ...,  4.0638e-01,\n                         -1.6003e-01, -2.2347e-01],\n                        [ 7.4877e-01, -6.4451e-01,  1.1027e+00,  ...,  1.6528e+00,\n                         -3.7494e-01,  5.9074e-02],\n                        [-1.8647e+00,  5.6722e-01, -1.9135e+00,  ..., -4.1777e+00,\n                         -2.9760e+00, -1.4171e+00],\n                        ...,\n                        [ 1.6897e+00, -4.5875e-03, -9.7279e-02,  ..., -3.2856e-01,\n                          8.4775e-01, -2.5544e-01],\n                        [ 9.8250e-01,  3.1623e+00,  5.2820e-01,  ...,  2.6850e-01,\n                         -5.1085e-01,  4.0425e-01],\n                        [-3.3155e-01, -2.2249e-01, -3.9446e-02,  ..., -9.5910e-03,\n                          1.4757e+00,  6.4930e-01]]]], grad_fn=<AddBackward0>))])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs = body5(images)\n",
    "body5outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys([0, 1, 2])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_values([tensor([[[[-1.0287, -0.2955, -0.8011,  ..., -0.6917, -0.3188, -0.1718],\n          [-0.8955, -0.4953, -0.2492,  ..., -1.3434, -0.6797, -0.5948],\n          [ 0.1749,  0.8077,  0.0800,  ..., -0.4142, -1.0549, -0.2239],\n          ...,\n          [-0.0838, -0.6984,  0.2127,  ...,  0.8075,  0.8470, -0.3681],\n          [ 0.0345, -0.4816,  0.2770,  ...,  0.3087,  0.5350,  0.1047],\n          [ 0.0804, -0.0887,  0.6788,  ...,  0.2320,  0.6985, -0.1012]],\n\n         [[-0.8315,  0.6266, -0.2660,  ..., -0.7062,  0.7801,  0.5138],\n          [ 0.4540, -1.8000, -0.7581,  ..., -1.2792,  0.1847, -1.1135],\n          [-0.1133, -1.6506, -0.6727,  ...,  1.6151,  0.3576,  0.6341],\n          ...,\n          [-0.3389,  0.7374,  0.2495,  ...,  0.4880,  0.3129,  0.1078],\n          [-0.3701,  1.2801, -0.0799,  ...,  0.4561,  0.3373,  0.3979],\n          [-0.4876,  0.8539,  0.1361,  ...,  0.3005,  0.3353,  0.2256]],\n\n         [[-0.2215,  0.2319,  0.1879,  ...,  1.5161, -0.6371,  0.1451],\n          [-0.2981, -1.3545,  1.5033,  ...,  0.7556, -0.0550,  0.6088],\n          [-0.6511, -0.2649,  0.6437,  ...,  0.2999,  0.6550,  0.1922],\n          ...,\n          [-0.1564, -0.7827,  0.6923,  ...,  0.2748,  1.0509,  0.7681],\n          [-0.5478, -0.0851,  0.5796,  ...,  0.4745,  0.4996,  0.6518],\n          [ 0.3699,  0.0244,  0.6017,  ...,  0.3085,  0.5105,  0.4283]],\n\n         ...,\n\n         [[ 0.0634,  0.5854,  0.9941,  ...,  0.8964,  1.3299,  1.5531],\n          [-0.6750, -0.6018,  0.3513,  ..., -0.1988, -0.4232,  0.1111],\n          [-0.0077, -0.1272,  1.1197,  ...,  0.4349, -0.6377,  0.1911],\n          ...,\n          [ 0.3183, -0.7791,  0.3330,  ...,  0.8526, -0.4304,  0.8313],\n          [ 0.2952, -0.9942,  0.2943,  ...,  0.1016,  0.1971,  0.7693],\n          [ 0.6752, -0.7148,  0.2436,  ..., -0.0936, -0.0708,  0.1546]],\n\n         [[ 1.1424, -1.0337,  0.0694,  ..., -0.5133,  0.7732,  0.1662],\n          [-0.0822,  0.3061, -0.6355,  ..., -0.5224,  0.2382,  0.4777],\n          [ 0.2244, -0.2425, -0.5626,  ...,  0.5499,  0.3178,  1.0116],\n          ...,\n          [-0.3516, -0.3008,  0.3606,  ..., -0.0081,  0.5906,  0.2738],\n          [ 0.3387, -0.3804,  0.3029,  ...,  0.1545,  0.2883,  0.4627],\n          [ 0.1185,  0.4604,  0.6399,  ...,  0.5155,  0.7655,  0.3558]],\n\n         [[-0.4309, -0.4202, -0.3691,  ..., -0.7153,  0.5310,  0.6808],\n          [ 0.7168, -1.2828, -0.1594,  ..., -1.1307,  1.2933,  0.0945],\n          [ 0.3065, -1.9876,  1.0138,  ...,  0.1900,  0.5670,  0.4609],\n          ...,\n          [ 0.1393, -0.0216,  0.6440,  ...,  0.6762,  0.5203,  0.4187],\n          [ 0.5787,  0.5075,  0.5131,  ...,  0.5465,  0.5530,  0.5576],\n          [ 0.7819,  0.8778,  1.1265,  ...,  1.0488,  1.0520,  0.3789]]],\n\n\n        [[[-1.1460, -0.0086, -0.9435,  ..., -0.4868, -0.8850, -2.3676],\n          [-2.4209,  1.3577, -2.2423,  ..., -0.5165,  0.4711,  0.2811],\n          [-1.8280, -0.4527, -1.1512,  ..., -1.0491,  0.1157,  0.0205],\n          ...,\n          [ 4.3264,  3.2576,  0.7792,  ...,  0.9579,  0.6284, -0.7256],\n          [ 1.5644, -0.0838, -0.2864,  ..., -0.6858,  0.0192, -0.2577],\n          [ 0.8961,  1.0824,  0.0340,  ...,  0.6214,  0.2506, -1.0681]],\n\n         [[ 1.7426,  0.8702, -1.1420,  ...,  2.7234,  1.7371, -0.1690],\n          [-1.9686, -3.6233, -5.3411,  ..., -2.6105, -3.8278, -4.7645],\n          [-0.2818, -1.9184,  1.2466,  ..., -0.3337, -0.4677, -0.7827],\n          ...,\n          [-4.3306, -3.6189,  1.3716,  ..., -1.3631, -1.4451, -0.6732],\n          [-2.9002, -2.5899, -0.2991,  ..., -1.6918, -0.8160, -1.4317],\n          [-5.4331, -2.5449, -0.6531,  ..., -0.1730, -0.1040, -0.6754]],\n\n         [[ 0.1000, -0.7292, -0.1229,  ..., -1.7251, -1.2660,  0.2984],\n          [-5.4692, -2.9505, -1.8133,  ..., -1.6670, -2.0202, -0.3307],\n          [-2.3113,  0.3050,  0.1196,  ..., -0.6474, -0.1054, -0.1248],\n          ...,\n          [-2.5566, -3.6462, -1.1961,  ..., -0.7776, -0.9033,  0.3349],\n          [-1.0529, -3.1207,  0.1760,  ..., -0.5122,  1.7838,  0.2734],\n          [-2.8754, -3.9855,  0.6290,  ..., -1.0319,  0.6856,  0.1964]],\n\n         ...,\n\n         [[ 0.3053, -1.1211, -0.0201,  ..., -0.0746,  1.1226, -0.1402],\n          [-1.1416, -2.2932, -0.9449,  ..., -1.2163, -1.1067, -3.4312],\n          [-1.3087, -0.0973, -1.0917,  ...,  0.2440,  0.8091,  0.0142],\n          ...,\n          [-4.3562, -2.9125, -1.8075,  ..., -1.2341,  0.1078, -0.4077],\n          [-2.9856, -1.8455,  0.0426,  ..., -0.9032, -0.0827,  0.2624],\n          [-2.6225, -0.1891, -0.4059,  ..., -2.6634,  1.6707, -0.5098]],\n\n         [[ 1.2235,  0.6893, -1.5773,  ..., -2.0042, -1.2727, -2.0661],\n          [ 2.2190,  1.1529,  0.6448,  ..., -1.8717, -2.2940,  0.9264],\n          [ 2.9375,  1.0229, -0.4757,  ...,  0.2873, -0.2784, -0.7856],\n          ...,\n          [ 1.2231, -2.3697, -0.4436,  ..., -1.1787, -0.8798, -1.3351],\n          [ 2.7396, -0.4413, -0.7323,  ...,  0.5778, -2.0671,  0.1106],\n          [ 2.4041,  0.7290,  0.2351,  ..., -0.0568,  0.1510,  0.6330]],\n\n         [[-1.2342, -1.0011, -1.3012,  ..., -2.6316, -2.1930, -1.6530],\n          [-1.8267, -0.3688, -0.6976,  ...,  1.3773,  0.1266, -0.5905],\n          [-1.9685, -0.4222, -0.1939,  ...,  0.3526, -0.3322, -0.9651],\n          ...,\n          [ 2.5133, -4.5988, -1.0577,  ..., -0.7032,  0.1345,  0.0588],\n          [ 1.9380, -1.6808, -0.0124,  ..., -2.9726, -1.8994,  0.4708],\n          [ 1.1411, -0.6895, -0.5149,  ..., -1.4069,  0.4963,  0.3225]]]],\n       grad_fn=<NativeBatchNormBackward>), tensor([[[[ 1.1172e+00,  7.4981e-01,  1.2046e+00,  ..., -3.1743e-02,\n            2.5929e-01,  5.1025e-03],\n          [ 1.4760e+00,  6.8694e-01,  2.6656e-01,  ...,  6.8918e-01,\n            4.5132e-05, -1.0145e-01],\n          [ 1.4698e+00,  8.6762e-01,  1.2058e+00,  ..., -6.6776e-01,\n            7.5373e-01, -9.8468e-02],\n          ...,\n          [ 4.5003e-01, -6.0279e-01, -8.0085e-01,  ..., -7.8238e-01,\n           -4.5939e-01, -7.4045e-02],\n          [ 5.7254e-01, -1.2816e-01, -1.6136e-01,  ..., -2.1314e-01,\n           -1.7074e-01, -1.7125e-02],\n          [ 4.5440e-01, -5.2869e-03, -3.9173e-01,  ..., -8.5364e-02,\n           -8.2608e-02,  5.8193e-02]],\n\n         [[ 2.6749e-01,  1.8167e-01, -4.2307e-01,  ..., -2.1237e-01,\n           -6.2031e-01, -7.0485e-01],\n          [ 6.4827e-01,  1.3112e-01,  7.8458e-01,  ...,  1.5867e-01,\n            8.3294e-02, -8.5470e-01],\n          [ 5.3363e-01, -7.9341e-01,  1.2851e-02,  ..., -9.4777e-01,\n            4.4539e-01,  5.5548e-01],\n          ...,\n          [ 6.6349e-01, -3.9594e-01, -8.0307e-01,  ..., -6.9387e-01,\n           -8.0722e-01, -4.8614e-01],\n          [ 3.8363e-01, -3.5606e-01, -2.6217e-01,  ..., -6.1895e-01,\n           -6.0896e-01, -5.3620e-01],\n          [ 4.4808e-03, -2.6094e-01, -1.8678e-01,  ..., -3.6214e-01,\n           -4.0434e-01, -2.4164e-01]],\n\n         [[ 1.7491e-01,  5.3317e-01,  5.5895e-01,  ..., -2.9013e-01,\n           -3.3383e-01, -4.0377e-01],\n          [-1.1576e-01,  1.8699e-01,  7.6024e-01,  ..., -4.1623e-01,\n           -3.5394e-01,  1.4107e-01],\n          [ 1.8757e-01,  9.1835e-01,  1.7958e+00,  ..., -1.1508e+00,\n            8.5222e-02, -7.9480e-01],\n          ...,\n          [-2.4218e-01, -5.5397e-01, -4.1694e-01,  ..., -6.6256e-01,\n           -9.3807e-01, -7.3335e-01],\n          [-1.8712e-01, -7.6005e-01, -5.8281e-01,  ..., -6.4202e-01,\n           -8.4059e-01, -5.4237e-01],\n          [-3.7486e-01, -7.8660e-01, -7.0951e-01,  ..., -6.9650e-01,\n           -6.7512e-01, -1.9945e-01]],\n\n         ...,\n\n         [[-7.2302e-01, -6.4162e-01, -6.2527e-01,  ...,  1.6937e-01,\n            3.6349e-02, -5.9554e-01],\n          [ 1.2312e-01, -7.3516e-01, -2.1544e-02,  ..., -2.2992e-01,\n           -1.1033e-01, -7.9800e-01],\n          [ 6.6940e-01,  1.1531e+00,  5.9935e-01,  ..., -1.2643e+00,\n           -8.9780e-01,  7.4302e-01],\n          ...,\n          [ 2.1477e-01, -8.3981e-02,  9.1120e-02,  ...,  7.4198e-02,\n           -8.0394e-03,  3.2258e-01],\n          [-2.2963e-01, -4.6281e-01, -9.3862e-02,  ..., -1.6741e-01,\n           -4.9481e-01,  4.2790e-02],\n          [ 2.2239e-01, -2.2373e-02,  1.8849e-01,  ..., -1.5082e-01,\n           -7.8535e-02,  4.9437e-02]],\n\n         [[ 2.5514e-01,  4.1261e-02,  4.2561e-01,  ...,  2.8126e-02,\n           -3.4343e-01,  9.6607e-02],\n          [ 1.5462e+00,  8.0926e-01,  7.5365e-01,  ..., -2.3063e-01,\n            1.5992e-01, -6.3655e-01],\n          [ 7.7001e-01,  6.0739e-01,  9.5067e-01,  ...,  1.9835e-01,\n            8.9161e-01, -7.9258e-01],\n          ...,\n          [ 9.7178e-02, -5.1052e-01, -4.6044e-01,  ..., -3.6277e-01,\n           -2.3927e-01,  1.8679e-01],\n          [ 1.8511e-01, -4.1707e-01, -5.7256e-01,  ..., -4.8846e-02,\n            1.4768e-02,  2.9032e-01],\n          [-2.5991e-01, -2.0361e-01, -7.4765e-02,  ..., -5.2902e-02,\n           -8.5396e-02,  2.3602e-01]],\n\n         [[ 1.8429e-01, -3.5593e-01, -4.9465e-01,  ...,  6.6760e-01,\n           -5.9303e-01,  1.7976e-01],\n          [-4.0298e-01, -5.7816e-01,  1.0592e+00,  ...,  4.5332e-01,\n           -8.3964e-02, -2.6185e-01],\n          [-2.3055e-01,  1.2753e+00,  1.6678e+00,  ...,  3.8045e-01,\n           -1.2317e-01,  7.4330e-01],\n          ...,\n          [ 2.8183e-01,  2.2476e-01,  4.6220e-01,  ...,  1.1424e-01,\n            1.4642e-01, -9.9893e-02],\n          [ 1.9206e-01,  3.6839e-01,  1.4861e-03,  ..., -9.1912e-02,\n            2.6664e-01, -5.4116e-02],\n          [ 2.0170e-01,  2.5296e-01,  2.9536e-02,  ...,  1.0873e-01,\n            2.5807e-01,  1.3572e-01]]],\n\n\n        [[[ 1.9900e+00,  5.1765e-02,  1.7125e+00,  ...,  1.2835e+00,\n           -1.6105e-01, -6.6695e-01],\n          [ 6.2994e-01,  2.0898e+00,  1.6947e+00,  ..., -7.3369e-01,\n            6.6773e-01,  4.5965e-01],\n          [ 1.2657e-01, -1.0415e+00,  3.1800e+00,  ...,  1.0279e+00,\n            1.0241e-02,  2.7184e-01],\n          ...,\n          [ 1.6799e-01, -4.0279e+00, -1.9414e+00,  ...,  4.7549e-01,\n           -5.4896e-01,  1.0898e+00],\n          [-7.5195e-01, -7.3816e-01, -1.9451e+00,  ...,  4.3247e-01,\n            6.3246e-01,  2.6599e-01],\n          [-8.0009e-01,  1.2214e+00, -1.5957e+00,  ...,  3.6704e-01,\n            2.6712e-01,  4.6265e-01]],\n\n         [[ 1.7575e+00,  3.9604e-02,  5.7028e-01,  ...,  5.0569e-01,\n           -9.1600e-01, -4.7875e-01],\n          [ 4.1478e-01, -5.2835e-01, -7.3475e-01,  ...,  8.8511e-01,\n           -1.6976e+00, -7.8147e-01],\n          [-1.6333e+00,  6.2205e-01, -2.3161e-01,  ...,  8.8553e-01,\n           -2.0004e-01,  3.8043e-01],\n          ...,\n          [-8.7323e-01, -1.8280e+00, -2.9440e+00,  ...,  1.3942e-01,\n           -2.3488e-01, -4.0187e-01],\n          [-1.6943e+00, -3.2042e+00,  2.8109e-01,  ..., -1.0525e-02,\n            2.4725e-01,  4.1285e-01],\n          [-3.5837e+00,  1.0943e+00, -1.7693e-01,  ...,  4.9952e-01,\n            3.2284e-01,  3.3321e-01]],\n\n         [[ 1.3631e+00,  8.7395e-01,  1.9501e+00,  ...,  1.6150e+00,\n            5.6555e-01, -1.3966e-01],\n          [ 5.5708e-01,  2.8404e+00,  2.1415e+00,  ..., -9.9308e-01,\n           -1.6287e+00, -1.0610e+00],\n          [ 6.5834e-01,  1.0221e+00,  3.8784e+00,  ...,  3.6102e-01,\n            3.1111e-01, -3.2842e-02],\n          ...,\n          [-9.5814e-01,  2.3862e-01, -9.3930e-01,  ..., -3.4848e-01,\n            9.0627e-01,  1.9039e-01],\n          [ 1.2494e+00,  8.2028e-01,  9.0438e-01,  ...,  2.4699e-01,\n           -8.8405e-01, -5.9188e-01],\n          [ 1.6400e+00,  4.3728e-01, -6.6048e-01,  ...,  1.4251e-01,\n            1.0364e+00, -1.0920e-01]],\n\n         ...,\n\n         [[ 1.4660e+00,  1.5815e-01,  2.5063e-01,  ...,  1.6219e+00,\n            2.1034e+00,  1.0430e+00],\n          [ 6.7504e-01, -1.5781e+00,  1.9398e+00,  ..., -1.2717e+00,\n           -3.0854e-01, -7.1286e-01],\n          [ 2.3650e+00,  2.8090e+00,  1.8964e+00,  ...,  4.6642e-01,\n            1.6101e+00, -1.8562e+00],\n          ...,\n          [ 5.9219e-01, -1.6804e+00, -3.5421e+00,  ...,  3.8854e-01,\n            6.0735e-01, -3.3260e-01],\n          [ 1.2847e-01,  3.8837e-01, -1.2657e+00,  ...,  1.0884e+00,\n            1.1005e+00,  8.0092e-01],\n          [-1.5151e+00,  1.4858e+00,  5.4771e-01,  ..., -1.5389e+00,\n            3.6289e-01,  1.5500e-01]],\n\n         [[ 4.2762e-02,  2.4055e+00,  7.1834e-01,  ..., -5.4720e-01,\n           -3.5952e-01,  3.3186e-01],\n          [-1.2750e+00, -3.8623e-01,  1.2904e+00,  ..., -1.1431e+00,\n           -4.8703e-01, -1.1221e+00],\n          [ 2.7821e+00, -2.6252e-01,  1.5799e+00,  ...,  6.8726e-02,\n           -6.4505e-01, -2.3601e-02],\n          ...,\n          [-1.8065e+00,  5.4329e-01,  4.0761e+00,  ..., -8.8423e-01,\n           -6.9876e-01, -5.3562e-01],\n          [-7.0744e-01,  7.6289e-01, -7.9512e-01,  ..., -2.0068e+00,\n           -1.3336e+00, -1.1010e+00],\n          [-7.2813e-01, -1.5471e+00, -6.7800e-01,  ..., -1.1903e+00,\n           -8.1946e-01, -7.7718e-01]],\n\n         [[-9.9440e-02, -5.6966e-01,  1.8694e-01,  ..., -7.5649e-01,\n           -1.2679e+00,  3.0182e-01],\n          [-1.6360e+00,  2.1977e-02,  5.4589e-01,  ..., -2.2600e+00,\n           -5.8394e-01,  7.8615e-01],\n          [-2.3373e+00,  1.7132e+00, -2.3140e+00,  ...,  3.2710e-01,\n            4.6386e-01,  1.4422e+00],\n          ...,\n          [-2.2469e+00, -2.8675e+00, -1.3352e+00,  ..., -2.4725e-01,\n            1.4316e-01, -2.9669e-01],\n          [-4.5405e+00,  1.5960e-01, -7.3387e-01,  ...,  3.6575e-01,\n           -2.3077e-01,  1.3253e+00],\n          [-4.5734e+00, -2.3272e-01, -9.8195e-01,  ...,  6.6379e-01,\n           -1.0771e+00,  1.0032e+00]]]], grad_fn=<NativeBatchNormBackward>), tensor([[[[ 2.6818e-01, -3.0713e-01,  9.3296e-01,  ..., -7.2070e-02,\n            5.6623e-01,  9.4032e-01],\n          [ 9.9202e-01, -3.1718e-01,  6.6551e-02,  ...,  3.5329e-01,\n            5.7826e-01,  1.2057e+00],\n          [ 8.3137e-02,  1.1910e+00,  3.4264e+00,  ...,  1.4008e-01,\n            5.2098e-01,  9.6077e-01],\n          ...,\n          [-2.6047e-02, -9.8307e-02,  3.4512e-02,  ..., -5.5310e-01,\n           -8.8283e-01,  3.9849e-01],\n          [ 4.1768e-01,  2.0427e-01, -8.1389e-03,  ..., -6.0479e-01,\n           -2.5219e-01, -4.5997e-01],\n          [ 7.4815e-01,  7.6613e-01,  8.2971e-01,  ...,  8.9338e-01,\n            4.1823e-01,  4.3994e-01]],\n\n         [[ 4.0596e-01,  1.2698e+00,  7.6740e-01,  ...,  2.6769e-01,\n           -1.8367e-01, -4.2464e-02],\n          [ 1.3783e+00,  9.6890e-01,  1.9408e+00,  ..., -9.9554e-01,\n            1.9038e-01, -2.1866e-01],\n          [-2.3551e-01,  9.7896e-01,  1.7036e+00,  ..., -2.9334e-01,\n            2.6411e-01, -8.7048e-04],\n          ...,\n          [-5.8164e-02,  1.9689e-01,  3.4615e-02,  ...,  5.4427e-01,\n            5.4060e-01,  2.2820e-01],\n          [ 1.0815e-01, -1.0593e-01,  4.1619e-01,  ...,  1.5443e-01,\n            1.8637e-01,  5.2127e-01],\n          [ 2.6987e-01,  2.9454e-01,  7.8332e-01,  ...,  6.3216e-01,\n            7.5313e-01,  7.2208e-01]],\n\n         [[-3.3481e-01,  3.2153e-01, -3.0891e-01,  ..., -7.1998e-01,\n            8.4450e-01,  8.6633e-01],\n          [-2.3619e-01,  1.1991e+00,  4.2846e-01,  ...,  5.3036e-01,\n            1.0558e+00,  1.6286e-01],\n          [ 2.8705e-01, -1.1575e+00,  2.1492e+00,  ...,  8.5733e-01,\n           -3.0811e-01,  1.5864e-01],\n          ...,\n          [ 9.6970e-02,  5.6615e-01,  6.2407e-01,  ...,  9.7744e-01,\n            6.3684e-01, -1.2539e-01],\n          [ 3.8559e-01,  5.3742e-01,  5.1831e-01,  ...,  8.1235e-01,\n            5.0940e-01, -3.5398e-02],\n          [ 7.2196e-01,  4.4926e-01,  7.2383e-02,  ...,  3.1538e-01,\n           -1.1604e-02,  4.9518e-02]],\n\n         ...,\n\n         [[-8.2825e-01,  8.5150e-02, -6.4795e-01,  ..., -1.5482e-01,\n           -1.0965e+00, -1.9694e-01],\n          [ 1.8386e-01,  8.7270e-01, -4.9010e-02,  ..., -1.1872e-01,\n           -6.6697e-01,  2.9162e-01],\n          [ 2.0702e+00,  4.6539e+00,  6.9686e-01,  ..., -6.2576e-02,\n            7.1149e-01,  1.4492e-01],\n          ...,\n          [-6.7642e-01, -8.2948e-01, -6.3322e-01,  ..., -4.1821e-01,\n            1.0440e-01,  9.7716e-01],\n          [-7.5116e-01, -7.7098e-01, -5.2579e-01,  ...,  7.0998e-01,\n            9.1478e-01,  6.2572e-01],\n          [-2.2435e-01, -1.1725e-01,  2.1530e-01,  ...,  1.2347e-01,\n            5.6903e-01,  2.5990e-01]],\n\n         [[ 4.6337e-01,  9.3301e-01,  1.3170e+00,  ...,  1.2774e-01,\n            1.4027e-01,  7.1617e-01],\n          [ 1.0716e+00, -6.2195e-01,  8.1885e-01,  ...,  7.8072e-01,\n            6.0195e-02,  1.0340e+00],\n          [ 2.1478e-01, -1.4699e+00, -3.0222e-01,  ...,  7.7277e-01,\n            5.5248e-01, -1.4620e-01],\n          ...,\n          [ 6.2491e-01,  9.2545e-01,  1.0776e+00,  ...,  6.2632e-01,\n            3.3433e-01,  1.2395e-01],\n          [ 2.4802e-01,  6.1924e-01,  7.3762e-01,  ...,  1.3609e-01,\n            3.0907e-01,  3.7686e-01],\n          [ 2.8122e-01,  4.1431e-01,  3.7476e-01,  ...,  3.5635e-01,\n            3.1058e-01,  3.8955e-01]],\n\n         [[-1.4604e-01, -3.5260e-01,  7.9725e-01,  ...,  8.2296e-01,\n            1.8789e-01,  3.6095e-01],\n          [ 1.6302e+00, -5.5932e-01,  1.9608e+00,  ...,  9.5154e-01,\n            7.8006e-01,  1.5199e+00],\n          [-1.9360e-01,  1.7770e+00, -5.2922e-02,  ...,  9.4788e-01,\n            1.0638e+00,  1.4937e+00],\n          ...,\n          [ 2.6404e-01,  4.2596e-01,  9.2848e-01,  ...,  1.8410e-01,\n            1.2291e+00,  9.9495e-01],\n          [ 6.5499e-01,  3.1627e-01,  2.8339e-01,  ...,  6.3482e-01,\n            6.7647e-01,  8.8502e-01],\n          [ 7.4596e-01,  7.1250e-01,  8.1583e-01,  ...,  1.0910e+00,\n            1.0307e+00,  8.0060e-01]]],\n\n\n        [[[ 3.9678e-02,  4.1267e-01,  1.2824e+00,  ...,  8.9393e-01,\n            7.2904e-01, -8.0427e-02],\n          [ 1.2885e+00, -5.8858e-01, -1.0585e+00,  ..., -4.8453e-01,\n           -3.3399e-01,  1.3410e+00],\n          [ 5.7542e-01, -3.0988e-02,  2.0391e+00,  ..., -1.6995e+00,\n           -1.0538e+00, -1.2840e+00],\n          ...,\n          [-4.7013e-01,  2.6503e+00, -4.7412e+00,  ..., -3.1497e-01,\n            6.2376e-01, -6.0068e-01],\n          [ 1.0676e+00, -1.0027e+00,  1.2860e+00,  ...,  6.2559e-01,\n           -2.5793e-01,  5.5135e-01],\n          [ 6.7295e-01,  1.1658e+00, -2.2603e+00,  ...,  1.0291e+00,\n            1.5602e+00,  6.5057e-01]],\n\n         [[-1.8994e-01,  1.0590e+00,  4.7198e-01,  ...,  6.6125e-01,\n           -2.1813e+00,  2.6573e-01],\n          [ 9.5922e-01,  3.3537e-01,  9.5515e-01,  ..., -1.0504e+00,\n           -1.0392e+00,  7.0789e-01],\n          [-2.1512e-01, -1.8410e+00, -1.1447e+00,  ..., -1.6028e+00,\n           -2.0107e-01,  4.9469e-01],\n          ...,\n          [-9.0959e-01, -4.2687e-01, -1.7341e+00,  ...,  6.2345e-02,\n            1.3490e+00, -5.7777e-01],\n          [-1.2466e+00,  2.1889e+00, -1.2522e+00,  ...,  7.6104e-02,\n            4.4413e-01,  4.3920e-02],\n          [-2.8657e-01,  1.6151e+00,  2.5330e+00,  ..., -3.9641e-01,\n            1.0055e+00, -2.2348e-01]],\n\n         [[-3.3373e-02,  9.3591e-01, -1.6262e-01,  ..., -1.8855e-01,\n           -8.3778e-01, -1.1317e-01],\n          [ 8.0296e-01, -2.1935e+00, -1.0744e+00,  ...,  2.4919e+00,\n           -7.5187e-01, -6.1645e-01],\n          [-4.0333e+00,  3.3154e+00,  3.4767e+00,  ..., -9.5695e-01,\n            1.1324e+00, -7.8642e-01],\n          ...,\n          [ 4.9674e-01, -1.0956e+00,  3.0219e-01,  ...,  2.2823e-01,\n            2.8386e-01, -5.7936e-01],\n          [ 1.9749e+00,  2.8471e+00,  1.0825e-01,  ...,  1.9658e+00,\n            2.5847e-01,  3.2147e-01],\n          [-8.6962e-01,  8.8421e-01, -1.9340e+00,  ...,  7.0978e-01,\n            1.3071e+00,  9.5451e-01]],\n\n         ...,\n\n         [[ 7.4017e-01, -3.5305e-01, -4.8652e-01,  ..., -8.2315e-01,\n           -8.5266e-01, -1.2359e-01],\n          [-4.3077e-02,  1.9267e+00, -8.8176e-01,  ...,  3.1720e-01,\n            9.4731e-01, -1.4925e+00],\n          [ 1.3360e+00, -3.2231e+00, -1.5778e+00,  ...,  1.4385e+00,\n            8.9146e-01, -8.7649e-01],\n          ...,\n          [-1.0526e+00, -8.5502e-01, -1.0995e-01,  ...,  6.8825e-02,\n           -4.6232e-01,  3.6191e-01],\n          [ 5.0905e-01,  1.6061e+00,  2.9704e+00,  ..., -1.4136e-01,\n           -6.4005e-01, -1.9797e-01],\n          [ 5.4744e-01,  9.7695e-01,  1.1172e+00,  ...,  5.7680e-01,\n           -6.8556e-02, -1.1736e+00]],\n\n         [[-7.0787e-01, -1.0261e-01, -1.4139e+00,  ..., -1.0090e+00,\n            3.6000e-01, -4.4108e-01],\n          [-1.0868e+00,  1.0141e-02,  3.6472e-01,  ...,  9.6627e-02,\n            1.9707e+00,  4.1770e-01],\n          [ 8.9345e-01, -3.4930e+00,  7.0642e-01,  ...,  2.2592e+00,\n           -2.3000e+00, -3.7969e-01],\n          ...,\n          [ 1.7990e+00, -9.2122e-01,  3.8469e+00,  ...,  1.1233e+00,\n            1.2448e+00,  2.4486e-01],\n          [-2.8070e-01, -3.9467e+00,  7.3385e-01,  ...,  7.2980e-01,\n            5.1622e-01, -6.6349e-02],\n          [-6.3312e-01, -3.5453e-01, -8.5869e-01,  ...,  8.4925e-01,\n           -3.5170e-02, -3.6832e-01]],\n\n         [[ 1.0652e+00,  7.0505e-01,  9.1074e-01,  ...,  4.0638e-01,\n           -1.6003e-01, -2.2347e-01],\n          [ 7.4877e-01, -6.4451e-01,  1.1027e+00,  ...,  1.6528e+00,\n           -3.7494e-01,  5.9074e-02],\n          [-1.8647e+00,  5.6722e-01, -1.9135e+00,  ..., -4.1777e+00,\n           -2.9760e+00, -1.4171e+00],\n          ...,\n          [ 1.6897e+00, -4.5875e-03, -9.7279e-02,  ..., -3.2856e-01,\n            8.4775e-01, -2.5544e-01],\n          [ 9.8250e-01,  3.1623e+00,  5.2820e-01,  ...,  2.6850e-01,\n           -5.1085e-01,  4.0425e-01],\n          [-3.3155e-01, -2.2249e-01, -3.9446e-02,  ..., -9.5910e-03,\n            1.4757e+00,  6.4930e-01]]]], grad_fn=<AddBackward0>)])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputs.values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "body5outputsvalues = list(body5outputs.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 40, 53, 53])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 48, 53, 53])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 96, 27, 27])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body5outputsvalues[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from models.retinaface import RetinaFace\n",
    "\n",
    "cfg = cfg_mnet\n",
    "\n",
    "mnet1 = RetinaFace(cfg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    mnet1outputs = mnet1(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(mnet1outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 4])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 2])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 10])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet1outputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from models.net import MobileNetV1 as MobileNetV1\n",
    "from models.net import FPN as FPN\n",
    "from models.net import SSH as SSH\n",
    "\n",
    "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_bn_no_relu(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride, leaky=0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "    )\n",
    "\n",
    "class SSH_mv3(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(SSH, self).__init__()\n",
    "        assert out_channel % 4 == 0\n",
    "        leaky = 0\n",
    "        if (out_channel <= 64):\n",
    "            leaky = 0.1\n",
    "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
    "\n",
    "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv3X3 = self.conv3X3(input)\n",
    "\n",
    "        conv5X5_1 = self.conv5X5_1(input)\n",
    "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
    "\n",
    "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
    "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
    "\n",
    "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)\n",
    "\n",
    "\n",
    "class RetinaFace_mobilenetv3sm(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace_mobilenetv3sm,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        backbone = torchvision.models.mobilenet_v3_small()\n",
    "        # backbone.features[4].block[3][0].stride=(1,1)\n",
    "        # backbone.features[9].block[3][0].stride=(1,1)\n",
    "        print(backbone)\n",
    "\n",
    "        self.body = _utils.IntermediateLayerGetter(backbone.features, {'4':0, '7':1, '11':2})\n",
    "        # in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            40,\n",
    "            48,\n",
    "            96,\n",
    "        ]\n",
    "        # out_channels = cfg['out_channel']\n",
    "        out_channels = 64\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=out_channels)\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=out_channels)\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=out_channels)\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "\n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        features = [feature1, feature2, feature3]\n",
    "        print(\"feature1.shape: \", feature1.shape)\n",
    "        print(\"feature2.shape: \", feature2.shape)\n",
    "        print(\"feature3.shape: \", feature3.shape)\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvBNActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): ConvBNActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): ConvBNActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnetv3 = RetinaFace_mobilenetv3sm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1.shape:  torch.Size([2, 64, 53, 53])\n",
      "feature2.shape:  torch.Size([2, 64, 53, 53])\n",
      "feature3.shape:  torch.Size([2, 64, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mnetv3outputs = mnetv3(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnetv3outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 12694, 4])"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 12694, 2])"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 66150, 10])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnetv3outputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "cfgres = cfg_re50\n",
    "\n",
    "res50fnet = RetinaFace(cfgres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1.shape:  torch.Size([2, 256, 105, 105])\n",
      "feature2.shape:  torch.Size([2, 256, 53, 53])\n",
      "feature3.shape:  torch.Size([2, 256, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    res50fnetoutputs = res50fnet(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res50fnetoutputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 4])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 2])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 29126, 10])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res50fnetoutputs[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([9, 15])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 15])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-493cc119",
   "language": "python",
   "display_name": "PyCharm (programmers assignment ai 1)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}